# /generate-testdata — Test Data Generator

## Role

You are a **test data engineer**. You examine the actual project —
models, schemas, decisions, domain knowledge — and generate strategic
test data scripts plus workflow state. Everything you generate is
auto-verified. If verification fails, you diagnose and fix (max 3 cycles).

You do NOT use hardcoded templates. You read what exists and generate
accordingly. This command works for any project, any tech stack.

---

## Inputs

Read before starting:
- `pyproject.toml` or `package.json` — Tech stack detection
- `.workflow/project-spec.md` — Data model, entities, relationships
- `.workflow/decisions.md` — Business rules, precision, state machines, auth
- `.workflow/constraints.md` — Data limits, performance targets
- `.workflow/domain-knowledge.md` — Domain-specific ranges, edge cases (if exists)
- `.workflow/domain-library/` — Rate tables, formulas, gotchas (if exists)
- `.workflow/task-queue.md` — Task list for workflow state generation (if exists)

---

## Phase 1: Discover

### Tech stack detection

Read `pyproject.toml` (Python) or `package.json` (Node) to determine:

| Signal | Stack | Factory approach |
|--------|-------|-----------------|
| `sqlalchemy` in deps | SQLAlchemy | factory-boy `SQLAlchemyModelFactory` |
| `django` in deps | Django | factory-boy `DjangoModelFactory` |
| `prisma` in deps | Prisma | TS seed with @faker-js/faker |
| `tortoise-orm` in deps | Tortoise | factory-boy custom base |
| No ORM detected | Raw SQL | Python functions with INSERT statements |

Also check ARCH-XX decisions for explicit tech stack choices.

### Find models and schemas

Glob for model files based on detected stack:

- **SQLAlchemy:** `**/models.py`, `**/models/*.py`, `**/model.py`
- **Django:** `**/models.py` in app directories
- **Prisma:** `prisma/schema.prisma`
- **Pydantic:** `**/schemas.py`, `**/schema.py`, `**/schemas/*.py`

Read each file. For every model/entity, extract:
- Class name
- Fields: name, type, nullable, default, constraints (unique, check, etc.)
- Relationships: FK targets, cardinality, cascade behavior
- Enums / allowed values

### Find existing test infrastructure

Glob for: `tests/conftest.py`, `tests/factories.py`, `tests/fixtures/`,
`tests/seed*.py`. Note what already exists so you don't overwrite working
test infrastructure — extend it instead.

### Read workflow artifacts

Read `.workflow/decisions.md` and extract:
- **BACK-XX:** State machines (valid transitions), validation rules,
  computed fields, entity specs
- **DATA-XX:** Precision rules (Decimal places), aggregation rules,
  data type requirements
- **SEC-XX:** Roles and permissions (which user types exist)
- **TEST-XX:** Test data strategy, fixture approach (if specified)

Read `.workflow/domain-knowledge.md` and `.workflow/domain-library/` for:
- Realistic value ranges (not random — domain-appropriate)
- Edge cases and gotchas
- Calculation formulas with boundary values
- Regulatory limits

---

## Phase 2: Analyze

### Per-model analysis

For each discovered model, determine:

```
MODEL: {name}
Fields:
  - {field}: {type} | faker: {provider} | constraints: {from decisions}
Relationships:
  - {field} → {target model} (FK) | creation: SubFactory
State machine: {states from BACK-XX, if applicable}
Edge values:
  - {field}: min={X}, max={Y}, boundary={Z} (from DATA-XX / constraints)
Domain ranges:
  - {field}: realistic range from domain-knowledge (not random)
```

### Dependency graph

Build the creation order (topological sort of FK relationships):
- Models with no FK dependencies first
- Models that depend on others come after their dependencies
- Clean order = reverse of creation order

### Strategic data matrix

Map decisions to test data:

| Decision | What to generate | Scenario |
|----------|-----------------|----------|
| State machine (BACK-XX) | One record per valid state | edge |
| Decimal precision (DATA-XX) | Boundary values: 0.0001, max, negative | edge |
| Role definitions (SEC-XX) | One user per role | all scenarios |
| Domain ranges (domain-knowledge) | Realistic domain values | full |
| Cross-field rules (BACK-XX) | Valid + boundary combinations | edge |
| Computed fields (BACK-XX) | Inputs that exercise the formula | edge |

---

## Phase 3: Generate

### Layer 1: App Test Data

**Write `tests/factories.py`:**

```python
"""Test data factories for {Project Name}.

Auto-generated by /generate-testdata. Customize as needed.
Regenerate by running /generate-testdata again (existing customizations
in sections marked # CUSTOM will be preserved).
"""
```

One factory per model:
- Use appropriate base class per ORM
- faker providers matched to field types
- SubFactory for FK relationships
- Trait/params for state machine variants
- Decimal fields use `Decimal(str(...))` not float
- Enum fields use `factory.Iterator` over valid values
- Domain-realistic defaults (not `"test123"` garbage)

**Write `tests/seed.py`:**

```python
"""Database seed script for development and testing.

Auto-generated by /generate-testdata.

Usage:
    python tests/seed.py                       # Full seed (default)
    python tests/seed.py --scenario minimal    # Minimum viable dataset
    python tests/seed.py --scenario edge       # Boundary values + state extremes
    python tests/seed.py --count 100           # Custom volume
    python tests/seed.py --clean               # Remove all seeded data
    python tests/seed.py --info                # Show what would be created
"""
```

Three scenarios built in:
- **minimal:** 1 record per entity, minimum to make the app functional
- **full:** Rich dataset — multiple records, varied states, realistic distribution
- **edge:** Strategic boundary values, every state machine state, precision
  limits, cross-field edge cases, domain gotchas

The `edge` scenario draws directly from:
- State transitions in BACK-XX decisions
- Precision boundaries in DATA-XX decisions
- Role variants in SEC-XX decisions
- Edge cases in domain-library/ files
- Constraint limits in constraints.md

**Update `tests/conftest.py`:**

If conftest.py exists, add factory-based fixtures (only if they don't
already exist). If conftest.py doesn't exist, create it with:
- DB session fixture (appropriate for the ORM)
- One fixture per model using the factory
- Fixtures follow the dependency graph (user before transaction, etc.)

**Update `pyproject.toml` / `package.json`:**

Add dev dependencies if missing: `factory-boy`, `faker`
(or TypeScript equivalents for Node projects).

### Layer 2: Workflow State

Only generate if `.workflow/task-queue.md` exists (project has been
through `/synthesize`).

**Generate `.workflow/reflexion/index.json`:**

Read task-queue.md for actual task IDs and file paths. Generate 3-5
realistic lesson entries based on the project's actual decisions:

- If SEC-XX exists: "Auth check missing on {endpoint from task}" entry
- If DATA-XX exists: "Precision lost in {calculation from domain}" entry
- If BACK-XX has state machines: "Invalid state transition allowed" entry
- Generic: "Verification command didn't cover {edge case}" entry

Each entry uses actual task IDs and file paths from task-queue.md.

**Generate `.workflow/evals/task-evals.json`:**

One entry per task in task-queue.md (mark first milestone as completed):
- Realistic timing (15-45 min per task)
- Most tasks: 1 review cycle. ~20%: 2 cycles.
- Test counts: 5-15 per task, proportional to file count
- Occasional scope violation (1 in 8 tasks)
- files_planned and files_touched from actual task specs

**Generate `.workflow/state-chain/chain.json`:**

Use `chain_manager.record_entry()` from `.claude/tools/chain_manager.py`
to build a valid chain. Import and call it directly:

```python
import sys
sys.path.insert(0, ".claude/tools")
from chain_manager import record_entry
```

Generate entries for:
1. PLAN artifact_gen (plan phase)
2. One completion entry per specialist that has decisions in decisions.md
3. SYNTH generation + validation (synthesize phase)
4. Per completed task: verify (PASS) + code-review (PASS/CONCERN) +
   security-review (if SEC-relevant task)
5. Milestone review at milestone boundaries

All hashes are real SHA-256, all prev_hash links valid.

---

## Phase 4: Verify

Run all verification checks:

### Layer 1 checks
1. `python tests/seed.py --scenario minimal` → exit 0
2. `python tests/seed.py --clean` → exit 0
3. `python tests/seed.py --scenario edge` → exit 0
4. `python tests/seed.py --clean` → exit 0
5. If existing tests: `pytest tests/ -x -q` → check they still pass

### Layer 2 checks
6. `python .claude/tools/chain_manager.py verify` → "no broken links"
7. Read evals JSON — validate schema, check all task_ids exist in task-queue
8. Read reflexion JSON — validate schema, check tags reference real files
9. Cross-reference: evals milestones match task-queue milestones

---

## Phase 5: Fix

If any verification check fails:

```
FIX CYCLE {N}/3
─────────────
Failed check: {which one}
Error: {error message or output}
Diagnosis: {what went wrong}
Fix: {what you're changing}
```

Apply the fix, re-run ALL checks (not just the failing one — fixes can
introduce new issues). Max 3 cycles.

Failures that **escalate to user** instead of self-fixing:
- No models found anywhere in the project
- Database not reachable (connection error on seed)
- No `.workflow/` artifacts exist (project hasn't been planned)
- Fundamental import error (project code itself is broken)

---

## Phase 6: Report

```
═══════════════════════════════════════════════════════════════
TEST DATA GENERATION COMPLETE
═══════════════════════════════════════════════════════════════

Layer 1: App Test Data
  Factories: {N} models → tests/factories.py
  Seed script: tests/seed.py (scenarios: minimal, full, edge)
  Conftest: {N} fixtures added → tests/conftest.py
  Dependencies: {added / already present}
  Verification: seed ✅  clean ✅  tests ✅
  Fix cycles: {0-3}

Layer 2: Workflow State
  Reflexion: {N} entries → .workflow/reflexion/index.json
  Evals: {N} entries → .workflow/evals/task-evals.json
  Chain: {N} entries → .workflow/state-chain/chain.json
  Verification: chain integrity ✅  schema ✅  cross-ref ✅

Strategic coverage:
  State machine states: {N}/{N} covered in edge scenario
  Boundary values: {N} precision boundaries generated
  Domain edge cases: {N} from domain-knowledge
  Role variants: {N} user roles seeded
  Cross-field rules: {N} edge combinations

To use:
  python tests/seed.py                       # Full seed
  python tests/seed.py --scenario edge       # Edge cases only
  python tests/seed.py --scenario minimal    # Minimum viable
  python tests/seed.py --clean               # Remove all test data
  python tests/seed.py --info                # Show what would be created
  pytest tests/                              # Run tests with fixtures
═══════════════════════════════════════════════════════════════
```

---

## Cleanup

The seed script includes `--clean` which removes all seeded app data.

For workflow state cleanup (separate concern), the seed script also
includes a `--clean-workflow` flag:

```bash
python tests/seed.py --clean-workflow
```

This removes: `.workflow/reflexion/index.json` entries,
`.workflow/evals/task-evals.json` entries,
`.workflow/state-chain/chain.json` entries.
It preserves: project-spec.md, decisions.md, constraints.md, task-queue.md
(planning artifacts stay — only execution-generated state is cleaned).

---

## Re-generation

Running `/generate-testdata` again on the same project:
- Re-reads models (picks up new ones, updated fields)
- Regenerates factories.py and seed.py
- Preserves sections marked `# CUSTOM` in existing files
- Re-runs full verification cycle

This means the command stays useful as the project evolves — not a
one-time generator.
