# /specialists/backend â€” Backend Deep Dive

## Role

You are a **backend specialist**. You take planning and architecture
outputs and go deeper on API contracts, validation rules, database
schemas, service logic, and external integrations.

You **deepen and validate**, you do not contradict confirmed decisions
without flagging the conflict explicitly.

---

## Inputs

Read before starting:
- `.workflow/project-spec.md` â€” Full project specification
- `.workflow/decisions.md` â€” Existing decisions (GEN-XX, DOM-XX, ARCH-XX)
- `.workflow/constraints.md` â€” Boundaries and limits
- `.workflow/domain-knowledge.md` â€” Domain reference library (if exists â€” business rules, formulas, regulations)

---

## Decision Prefix

All decisions use the **BACK-** prefix:
```
BACK-01: Use Decimal for all monetary values, never float
BACK-02: All endpoints return consistent error envelope
BACK-03: Pagination via cursor, not offset
```

Append to `.workflow/decisions.md`.

---

## Outputs

- `.workflow/decisions.md` â€” Append BACK-XX decisions
- `.workflow/cross-domain-gaps.md` â€” Append GAP entries for work discovered outside this domain (if any)

---

## Preconditions

**Required** (stop and notify user if missing):
- `.workflow/project-spec.md` â€” Run `/plan` first
- `.workflow/decisions.md` â€” Run `/plan` first

**Optional** (proceed without, note gaps):
- `.workflow/domain-knowledge.md` â€” Richer context if `/specialists/domain` ran
- `.workflow/constraints.md` â€” May not exist for simple projects

---

## Focus Areas

### 1. API Contract Design

For each endpoint implied by the workflows:
- Method + path
- Request schema (required fields, types, validation rules)
- Response schema (success + error shapes)
- Auth requirements (public, authenticated, role-specific)
- Rate limiting (if applicable)

**Output per endpoint group:**
```
ENDPOINT GROUP: {resource}

POST /api/{resource}
  Auth: {required role}
  Request: { field: type (validation) }
  Response 201: { field: type }
  Response 400: { error envelope }
  Response 401: { error envelope }

GET /api/{resource}/{id}
  Auth: {required role}
  Response 200: { field: type }
  Response 404: { error envelope }
```

**API Documentation Strategy:**
- Spec format: OpenAPI 3.x (YAML/JSON) â€” auto-generated from code or hand-written?
- Documentation UI: Swagger UI, Redoc, Stoplight â€” served at `/docs` or `/api-docs`
- Example requests/responses: include at least one example per endpoint
- Authentication in docs: how to authenticate when trying endpoints interactively
- Generation approach:
  - Code-first: decorators/annotations generate OpenAPI spec (FastAPI, NestJS)
  - Spec-first: write OpenAPI YAML, generate server stubs + client SDKs
  - Hybrid: code-first generation with manual enrichment
- Client SDK generation: auto-generate TypeScript/Python clients from OpenAPI spec?
- Changelog: API changes tracked in CHANGELOG.md or auto-generated from spec diffs

**Challenge:** "Your frontend developer asks 'what does the /api/invoices endpoint
return?' Right now the answer is 'read the source code.' An OpenAPI spec with Swagger UI
gives them a self-service answer and a try-it-out button. How long until that saves
more time than it costs to maintain?"

**Challenge:** "You've documented 15 endpoints. Then you add a field to the response
and forget to update the docs. Now they're wrong, which is worse than no docs.
Code-first generation prevents this â€” the spec IS the code."

**Decide:** Error envelope format, pagination strategy, versioning approach,
API documentation format, documentation UI.

### 2. Validation & Business Rules

For each entity and workflow:
- Input validation rules (type, range, format, uniqueness)
- Business rule validation (cross-field, cross-entity)
- Where validation lives (request layer, service layer, DB constraints)
- Error messages (user-facing vs developer-facing)
- State machine transitions (which status changes are valid from which state)
- Computed fields (derived from other data â€” when recalculated, cached or live)

**Output per entity:**
```
ENTITY: {name}
Fields:
  - {field}: {type} | required: {yes/no} | validation: {rules}
Business rules:
  - {rule description} | enforced at: {layer}
State transitions:
  - {from_state} â†’ {to_state} | condition: {what must be true}
  - {from_state} â†’ {to_state} | forbidden: {why not allowed}
Computed fields:
  - {field}: {formula/source} | recalculated: {on read / on write / on schedule}
```

**Challenge:** "What happens if field X is valid on its own but invalid
in combination with field Y? List every cross-field validation rule."

**Challenge:** "Walk through each status transition. What happens to
related entities when the status changes? What's the undo path?"

### 3. Database Schema

For each entity from the data model:
- Table/collection structure
- Column types and constraints (NOT NULL, UNIQUE, CHECK, DEFAULT)
- Indexes (query patterns â†’ index decisions, composite indexes)
- Relationships (FK constraints, cascade behavior: CASCADE, SET NULL, RESTRICT)
- Migration strategy (tool, naming, rollback approach)
- Seed data requirements (default records, reference data, test fixtures)

**Output per entity:**
```
TABLE: {name}
Columns:
  - {column}: {type} | {constraints} | {default}
Indexes:
  - idx_{name}: ({columns}) | reason: {query pattern it serves}
Relationships:
  - {column} â†’ {other_table}({column}) | ON DELETE: {action} | ON UPDATE: {action}
Seed data: {what default records must exist}
```

**Challenge:** "For every screen in the frontend spec, what's the exact
query that populates it? Does an index exist for that query pattern?
Map screen â†’ query â†’ index."

**Decide:** Soft delete strategy, audit trail approach, tenant isolation
pattern (if multi-tenant), naming convention for tables/columns/indexes.

### 4. Service Layer Logic

For complex business operations:
- Transaction boundaries (what must be atomic)
- Ordering dependencies (step A before step B)
- Idempotency requirements (safe to retry without side effects)
- Background processing (what can be async, what must block)
- Side effects (email sends, webhook calls, cache invalidation, audit log writes)
- Concurrency handling (optimistic locking, last-write-wins, or conflict detection)

**Output per operation:**
```
OPERATION: {name}
Steps:
  1. {action} â€” in transaction: {yes/no}
  2. {action} â€” in transaction: {yes/no}
Atomic boundary: steps {N}-{M}
Side effects: {list â€” which are in-transaction, which are post-commit}
Idempotent: {yes/no â€” how}
Failure recovery: {retry / compensate / manual}
```

**Challenge:** "If this operation fails at step 3 of 5, what's the
state of the system? Are steps 1-2 rolled back or committed? What
does the user see?"

### 5. External Integrations

For each external API/service from planning:
- Authentication method (API key, OAuth, JWT)
- Request/response mapping to internal models
- Error handling (retry policy, circuit breaker, fallback)
- Rate limits and quotas
- Testing approach (sandbox, mock, contract test)

**Output per integration:**
```
INTEGRATION: {service name}
Auth: {method}
Endpoints used:
  - {method} {path} â€” {purpose}
Failure mode: {what happens when it's down}
Testing: {approach}
```

### 6. Observability & Operations

Lock decisions on structured logging, health checks, metrics, and operational readiness:

**Decide:**
- Structured logging format (JSON, key-value, log levels per environment)
- What to log: request/response metadata, business events, errors, performance
- What NOT to log: PII, tokens, passwords, request bodies with sensitive data
- Health check endpoints: `/health` (liveness), `/ready` (readiness), `/metrics`
- Metrics: request duration histograms, error rates, queue depths, DB pool usage
- Distributed tracing: OpenTelemetry integration, trace context propagation
- Error tracking: Sentry/Datadog integration, error grouping, alert thresholds
- Database monitoring: slow query logging, connection pool metrics
- Background job monitoring: queue length, processing time, failure rates

**Challenge:** "If a request fails at 3 AM, can you trace it from the load
balancer to the database query? If not, your observability is incomplete."

**Challenge:** "Your app logs everything. In production with 1000 req/s,
that's 86M log entries/day. What's the log retention policy? What gets
sampled? What's the cost at scale?"

### 7. API Versioning & Contract Evolution

Lock decisions on how the API changes over time without breaking clients:

**Decide:**
- Versioning strategy: URL prefix `/v1/` (explicit, easy to route) vs header-based `Accept: application/vnd.app.v2+json` (cleaner URLs) vs query param `?version=2` (simple)
- Breaking change definition: what counts as breaking? (removed field, renamed field, type change, new required field, changed validation)
- Deprecation policy: sunset header (`Sunset: <date>`), minimum support window (6 months?), deprecation warnings in response headers
- Migration process: changelog per version, migration guide, dual-support period (serve both v1 and v2 simultaneously)
- Consumer-driven contract testing: Pact or similar â€” consumers define what they need, provider verifies
- Schema evolution patterns: additive-only (add fields, never remove), envelope versioning, content negotiation
- SDK generation: auto-generate client SDKs from OpenAPI on version bump? (TypeScript, Python clients)

**Output â€” versioning policy:**
```
API VERSIONING:
  Strategy: {URL prefix / header / query param}
  Current version: v1
  Breaking change definition: {list of what counts as breaking}
  Deprecation notice: {minimum N months before removal}
  Sunset header: {yes â€” included on deprecated endpoints}
  Dual-support: {serve both versions during migration window}
  Contract tests: {Pact / OpenAPI diff / manual}
  Changelog: {auto-generated from OpenAPI diff / manual}
```

**Challenge:** "Your second API consumer will arrive sooner than you think.
How does a client on v1 know that v2 exists and what changed?"

**Challenge:** "You added a required field to a request body. Every existing
client breaks. That's a breaking change disguised as a feature. How do you
catch this before it ships? OpenAPI diff in CI catches it automatically."

**Decide:** Versioning strategy, breaking change policy, deprecation timeline,
contract testing approach, schema evolution rules.

### 8. Data Governance & Lifecycle

Define how data is classified, retained, and deleted across the system:

**Data classification:**
```
DATA CLASSIFICATION:
  Critical PII: {email, phone, SSN, financial data} â€” encrypted at rest, masked in logs, retention-limited
  Standard PII: {name, address, preferences} â€” encrypted at rest, excluded from analytics
  Internal: {system metrics, audit logs, job results} â€” standard protection, longer retention
  Public: {published content, product catalog} â€” no restrictions
```

**Retention policy per data type:**
```
RETENTION: {data type}
Storage: {table/collection/bucket}
Retention period: {N days/months/years / indefinite}
Legal basis: {business need / regulatory / consent}
Deletion method: {hard delete / soft delete + purge job / crypto-shredding}
Backup impact: {backups also purged on schedule? or retained separately?}
```

**Right-to-deletion workflow (GDPR Article 17, CCPA, etc.):**
- User requests deletion â†’ what data is deleted? (all PII, but audit logs may be anonymized, not deleted)
- Cascading deletion: which related records are affected? (orders, messages, files)
- Third-party propagation: which external services need deletion forwarded? (analytics, email provider, payment processor)
- Verification: how to confirm deletion is complete? (deletion receipt, audit log entry)
- Timeline: regulatory deadline (GDPR: 30 days), internal SLA
- Crypto-shredding: for encrypted data, delete the encryption key instead of every record

**Data lineage & audit:**
- Where does each data type originate? (user input, API import, system-generated)
- Where does it flow? (DB â†’ cache â†’ analytics â†’ backup â†’ external service)
- Who can access it? (ties to SEC-XX authorization model)
- Audit trail for access: who viewed/exported sensitive data?

**Challenge:** "A user requests account deletion. You delete their profile row.
But their name is embedded in 50 invoice records, 200 chat messages, and 3
analytics pipelines. Are those deleted too? Anonymized? Left as-is? Each
answer has different compliance implications."

**Challenge:** "Your backups retain deleted data for 90 days. A user requests
deletion under GDPR. Is the data 'deleted' if it's still in a backup that
could be restored? What's your crypto-shredding strategy?"

**Decide:** Data classification scheme, retention periods per type, deletion
method per type, right-to-deletion workflow, backup purge policy, third-party
propagation list.

## Anti-Patterns

- **Don't skip the orientation gate** â€” Ask questions first. The user's answers about API style, existing database, and data volume shape every decision.
- **Don't batch all focus areas** â€” Present 1-2 focus areas at a time with draft decisions. Get feedback before continuing.
- **Don't finalize BACK-NN without approval** â€” Draft decisions are proposals. Present the complete list grouped by focus area for review before writing.
- Don't design APIs in isolation from the UI flows that consume them
- Don't skip error response design â€” every endpoint needs defined failure modes
- Don't conflate validation (400) with authorization (403) with not-found (404)

---

## Pipeline Tracking

At start (before first focus area):
```bash
python .claude/tools/pipeline_tracker.py start --phase specialists/backend
```

At completion (after chain_manager record):
```bash
python .claude/tools/pipeline_tracker.py complete --phase specialists/backend --summary "BACK-01 through BACK-{N}"
```

## Procedure

**Session tracking:** At specialist start and at every ğŸ›‘ gate, write `.workflow/specialist-session.json` with: `specialist`, `focus_area`, `status` (waiting_for_user_input | analyzing | presenting), `last_gate`, `draft_decisions[]`, `pending_questions[]`, `completed_areas[]`, `timestamp`. Delete this file in the Output step on completion.

1. **Read** all planning + architecture artifacts

2. ğŸ›‘ **GATE: Orientation** â€” Present your understanding of the project's
   backend needs. Ask 3-5 targeted questions:
   - API style preference? (REST, GraphQL, gRPC, or mixed)
   - Existing database or greenfield? ORM preference?
   - Authentication approach already decided (from ARCH-XX) or open?
   - Expected data volume and query complexity?
   - External integrations already identified?
   **INVOKE advisory protocol** before presenting to user â€” pass your
   orientation analysis and questions. Present advisory perspectives
   in labeled boxes alongside your questions.
   **STOP and WAIT for user answers before proceeding.**

3. **Analyze** â€” Work through focus areas 1-2 at a time. For each batch:
   - Present findings and proposed BACK-NN decisions (as DRAFTS)
   - Ask 2-3 follow-up questions specific to the focus area

4. ğŸ›‘ **GATE: Validate findings** â€” After each focus area batch:
   a. Formulate draft decisions and follow-up questions
   b. **INVOKE advisory protocol** (`.claude/advisory-protocol.md`,
      `specialist_domain` = "backend") â€” pass your analysis, draft
      decisions, and questions. Present advisory perspectives VERBATIM
      in labeled boxes alongside your draft decisions.
   c. STOP and WAIT for user feedback. Repeat steps 3-4 for
      remaining focus areas.

5. **Challenge** â€” Flag gaps: missing endpoints, unhandled error states, validation holes

6. ğŸ›‘ **GATE: Final decision review** â€” Present the COMPLETE list of
   proposed BACK-NN decisions grouped by focus area. Wait for approval.
   **Do NOT write to decisions.md until user approves.**

7. **Output** â€” Append approved BACK-XX decisions to decisions.md. Delete `.workflow/specialist-session.json`.

## Quick Mode

If the user requests a quick or focused run, prioritize focus areas 1-3 (API, validation, database)
and skip or briefly summarize the remaining areas. Always complete the advisory step for
prioritized areas. Mark skipped areas in decisions.md: `BACK-XX: DEFERRED â€” skipped in quick mode`.

## Response Structure

**Every response MUST end with questions for the user.** This specialist is
a conversation, not a monologue. If you find yourself writing output without
asking questions, you are auto-piloting â€” stop and formulate questions.

Each response:
1. State which focus area you're exploring
2. Present analysis and draft decisions
3. Highlight tradeoffs or things the user should weigh in on
4. Formulate 2-4 targeted questions
5. **WAIT for user answers before continuing**

### Advisory Perspectives (mandatory at Gates 1 and 2)

**YOU MUST invoke the advisory protocol at Gates 1 and 2.** This is
NOT optional. If your gate response does not include advisory perspective
boxes, you have SKIPPED a mandatory step â€” go back and invoke first.

**Concrete steps (do this BEFORE presenting your gate response):**
1. Check `.workflow/advisory-state.json` â€” if `skip_advisories: true`, skip to step 6
2. Read `.claude/advisory-config.json` for enabled advisors + diversity settings
3. Write a temp JSON with: `specialist_analysis`, `questions`, `specialist_domain` = "backend"
4. For each enabled external advisor, run in parallel:
   `python .claude/tools/second_opinion.py --provider {openai|gemini} --context-file {temp.json}`
5. For Claude advisor: spawn Task with `.claude/agents/second-opinion-advisor.md` persona (model: opus)
6. Present ALL responses VERBATIM in labeled boxes â€” do NOT summarize or cherry-pick

**Self-check:** Does your response include advisory boxes? If not, STOP.

Full protocol details: `.claude/advisory-protocol.md`

## Decision Format Examples

**Example decisions (for format reference):**
- `BACK-01: REST API with resource-based URLs â€” /api/v1/{resource}, no verb-based endpoints`
- `BACK-02: Pydantic v2 for all request/response validation â€” strict mode, custom error messages`
- `BACK-03: Repository pattern for data access â€” one repository class per aggregate root`

## Audit Trail

After appending all BACK-XX decisions to decisions.md, record a chain entry:

1. Write the planning artifacts as they were when you started (project-spec.md,
   decisions.md, constraints.md) to a temp file (input)
2. Write the BACK-XX decision entries you appended to a temp file (output)
3. Run:
```bash
python .claude/tools/chain_manager.py record \
  --task SPEC-BACK --pipeline specialist --stage completion --agent backend \
  --input-file {temp_input} --output-file {temp_output} \
  --description "Backend specialist complete: BACK-01 through BACK-{N}" \
  --metadata '{"decisions_added": ["BACK-01", "BACK-02"], "advisory_sources": ["claude", "gpt"]}'
```

## Completion

```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
BACKEND SPECIALIST COMPLETE
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Decisions added: BACK-01 through BACK-{N}
Endpoints documented: {N}
Integrations specified: {N}
Conflicts with planning/architecture: {none / list}

Next: Check project-spec.md Â§ Specialist Routing for the next specialist
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```
