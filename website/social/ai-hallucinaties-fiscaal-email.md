**Subject:** Stanford tested premium legal AI tools. The results should worry you.
**Preview:** Even Westlaw AI hallucinated on 17–33% of queries. Here's what that means for tax practice.

Stanford researchers found that even the most expensive RAG-based legal AI tools — Westlaw AI, Lexis+ AI — fabricate information on 17–33% of queries. General-purpose models like ChatGPT hit 58–88% on legal questions.

In our latest article, we explain why hallucinations aren't getting fixed (they're getting harder to detect), what makes Belgian tax law especially vulnerable, and the three architectural defenses that actually reduce the risk.

Key framework: **the confidence-competence inversion** — as models get better at language, they get worse at signaling when they don't know. Every generation makes hallucinations more dangerous, not less.

[Read the full article →]
