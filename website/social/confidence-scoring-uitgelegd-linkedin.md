Ask ChatGPT a well-documented tax question. You get a clear, authoritative answer.

Ask it a question it can't possibly know the answer to. You get the exact same tone. Same structure. Same confidence.

That's not a bug. It's how language models work.

Research shows LLMs overestimate their own correctness by 20-60%. The harder the question, the worse the calibration gets. And RLHF training — the process that makes models sound helpful — literally teaches them that confidence is rewarded.

For tax professionals, this is the most dangerous property of AI tools. You get no signal about which answers to trust and which to scrutinize.

Confidence scoring is the architectural response. Not model self-assessment (unreliable), but evidence-based scoring: how many sources were found, how authoritative they are, and whether they agree.

Two dimensions:
- Source coverage: Did it find statutes, case law, and circulars? Or just one tangentially related document?
- Reasoning certainty: Do the sources agree? Conflict? Address the exact question?

High confidence: act on it, verify key sources. Low confidence: full professional judgment required.

The difference between a tool that says "here's the answer" and one that says "here's the answer, and here's how much evidence supports it."

Read the full article: https://auryth.com/en/blog/confidence-scoring-uitgelegd-en/

#LegalAI #ConfidenceScoring #AIUncertainty #TaxTechnology #ProfessionalJudgment
