Every legal AI vendor claims high accuracy. Ask them for the data, and the conversation gets quiet.

Stanford researchers tested the leading legal AI tools. Despite "hallucination-free" marketing, every tool hallucinated 17-33% of the time. Meanwhile, the Foundation Model Transparency Index dropped from 58/100 to 40/100 in a single year.

The industry's rational strategy? Claim accuracy loudly, measure it never.

We think that's backwards. We publish our accuracy metrics — citation accuracy, substantive correctness, and completeness — continuously, not once. When our scores dip after a programme law, that dip is visible. A score that never changes proves only that nobody is checking.

Accuracy without methodology is just a number. Methodology without publication is just a promise.

Full article: [link]

#FiscaleAI #Transparantie #LegalTech #Auryth #AIBenchmarking
