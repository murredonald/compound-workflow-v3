---
title: "AI-hallucinaties: waarom ChatGPT bronnen verzint (en hoe u het herkent)"
description: "Waarom taalmodellen juridische bronnen fabriceren, wat Belgisch fiscaal recht extra kwetsbaar maakt, en drie verdedigingslagen die werken."
publishDate: 2026-02-13
author: "Auryth Team"
category: "ai-uitgelegd"
category_name: "AI uitgelegd"
tags: ["AI-hallucinaties", "ChatGPT", "juridische AI", "fiscale technologie", "AI-betrouwbaarheid"]
locale: "nl"
draft: false
---

Stanford-onderzoekers testten de duurste juridische AI-tools op de markt — Westlaw AI, Lexis+ AI — en ontdekten dat ze bij 17–33% van de zoekopdrachten informatie fabriceren. Algemene modellen zoals ChatGPT? Tussen 58% en 88%.

Die cijfers zijn geen bugs op een roadmap. Het zijn kenmerken van systemen die nooit ontworpen zijn om juridisch feit van plausibele fictie te onderscheiden.

## Wat is een AI-hallucinatie?

**Een [AI-hallucinatie](/nl/glossary/hallucination/)** treedt op wanneer een taalmodel output genereert die gezaghebbend klinkt maar feitelijk onjuist is — verzonnen juridische [referenties](/nl/glossary/citation/), onbestaande artikelnummers, gefabriceerde statistieken, of foutieve belastingtarieven met absolute stelligheid gepresenteerd. Het model liegt niet. Het voorspelt het meest waarschijnlijke volgende woord op basis van patronen in zijn trainingsdata. Wanneer die patronen de specifieke Belgische bepaling die u nodig hebt niet bevatten, vult het de leemte op met iets dat *juist klinkt*.

Taalmodellen zoeken geen informatie op. Ze genereren plausibele tekst. Het verschil is de kloof tussen een bibliothecaris die de plank controleert en een collega die uit het geheugen antwoordt — en nooit toegeeft wanneer die gokt.

## Waarom fiscaal recht hallucinatieterrein is

Niet alle domeinen zijn even kwetsbaar. Vraag een taalmodel om een nieuwsartikel samen te vatten, en hallucinaties zijn een ongemak. Vraag het naar Art. 344 WIB — de algemene antimisbruikbepaling — en u bevindt zich in een mijnenveld.

Drie factoren maken fiscaal recht bijzonder gevaarlijk:

**Precisieafhankelijkheid.** Het verschil tussen 0,12% en 1,32% TOB is het verschil tussen correct advies en een aansprakelijkheidsclaim. Taalmodellen optimaliseren voor plausibele tekst, niet voor precieze cijfers. "Dicht genoeg" bestaat niet in professioneel fiscaal werk.

**Referentiedichtheid.** Een enkel Belgisch fiscaal antwoord kan Art. 19bis WIB, de Vlaamse Codex Fiscaliteit, een voorafgaande beslissing van de DVB én een Fisconetplus-circulaire vereisen — tegelijkertijd. Algemene AI heeft de meeste van deze documenten nooit verwerkt. Dus genereert het referenties die *eruitzien* als echte.

**Temporele instabiliteit.** Belgisch fiscaal recht verandert voortdurend. Het vennootschapsbelastingtarief, TOB-drempels, regionale successierechten — allemaal bewegende doelen. Een taalmodel dat zes maanden geleden getraind is, geeft u het recht van gisteren met het vertrouwen van vandaag.

![Vier veelvoorkomende mythes over AI-hallucinaties in juridisch werk tegenover de realiteit](/blog/ai-hallucinaties-fiscaal/ai-hallucinatie-mythes-vs-realiteit-dark.png)

## De vijf signalen: hoe u een gehallucineerd fiscaal antwoord herkent

Hallucinaties laten vingerafdrukken achter. Na analyse van honderden AI-gegenereerde fiscale antwoorden komen vijf patronen consistent naar voren:

| Signaal | Hoe het eruitziet | Voorbeeld |
|---------|------------------|-----------|
| **Zekerheid zonder bron** | Stellig antwoord, geen artikelverwijzing | "Het TOB-tarief is 0,35%" — welk instrument? Onder welke voorwaarden? |
| **De te perfecte verwijzing** | Plausibel klinkend artikel dat niet bestaat | Een artikelnummer met paragrafen dat lijkt op echt Belgisch fiscaal recht maar niet in het WIB 92 te vinden is |
| **Jurisdictievermenging** | Regels uit het verkeerde land als Belgisch gepresenteerd | Nederlandse bronheffingsregels toegepast op een Belgische belastingplichtige |
| **Temporele blindheid** | Huidige tarieven voor een historische vraag | Het vennootschapsbelastingtarief van 2026 voor aanslagjaar 2019 |
| **Ontbrekende nuances** | Zuiver antwoord waar de wet complex is | Eén TOB-tarief terwijl er drie gelden afhankelijk van fondsclassificatie |

Het laatste signaal is het gevaarlijkst. Een verzonnen artikelnummer is gemakkelijk te ontdekken — u zoekt het op en het bestaat niet. Een onvolledig antwoord dat compleet *klinkt*? Daar verbranden professionals zich en verliezen cliënten geld.

> Een model dat nooit "ik weet het niet" zegt, liegt vaker dan u denkt.

## De vertrouwen-competentie-inversie

Hier is de tegenintuïtieve waarheid over AI-vooruitgang: **naarmate modellen beter worden in taal, worden ze slechter in aangeven wanneer ze het niet weten.**

GPT-3 hallucineerde opvallend — onhandige tekst, zichtbare fouten. GPT-4 hallucineert welsprekend. De verzonnen juridische verwijzing komt verpakt in vloeiend Belgisch juridisch Nederlands, compleet met voorwaarden en uitzonderingen die *lijken op* echt fiscaal recht.

OpenAI-onderzoekers documenteerden deze dynamiek in 2025: trainingsdoelstellingen belonen zelfverzekerde [voorspelling](/nl/glossary/inference/) boven eerlijke onzekerheid. Het model dat zegt "ik weet het niet zeker" wordt bestraft in benchmarks. Het model dat een plausibel antwoord verzint, wordt beloond.

Dit is geen bug die gepatcht kan worden. Xu et al. bewezen in 2024 formeel dat hallucinatie *wiskundig onvermijdelijk* is voor taalmodellen die als algemene probleemoplossers worden ingezet. Niet moeilijk te elimineren. Niet een tijdelijke beperking. Onmogelijk — per bewijs.

Wij noemen dit de **vertrouwen-competentie-inversie**: hoe beter de taal, hoe moeilijker het wordt om kennis van fabricatie te onderscheiden. Elke modelgeneratie maakt hallucinaties gevaarlijker, niet minder.

![Kerncijfers over AI-hallucinatiepercentages en juridische gevolgen](/blog/ai-hallucinaties-fiscaal/ai-hallucinatie-cijfers-dark.png)

## Maar RAG lost dit toch op?

Gedeeltelijk. [Retrieval-Augmented Generation](/nl/glossary/rag/) — waarbij de AI echte documenten doorzoekt voordat het antwoordt — vermindert hallucinaties aanzienlijk. Het Stanford-team vond dat RAG-gebaseerde juridische tools hallucineren bij 17–33% van de zoekopdrachten, tegenover 58–88% bij algemene modellen. Dat is echte vooruitgang.

Maar 17% is niet nul. Eén op zes zoekopdrachten die onjuiste informatie oplevert, is geen afrondingsfout — het is een professioneel risico. En de resterende hallucinaties zijn het moeilijkst te detecteren: ze citeren bronnen die echt lijken, matchen het formaat van correcte antwoorden, en geven geen signaal dat er iets fout is.

De Orde van Vlaamse Balies erkende deze realiteit in haar AI-richtlijnen: advocaten moeten alle AI-output kritisch verifiëren, inclusief geciteerde bronnen en rechtspraak. De professionele verantwoordelijkheid blijft bij u, ongeacht welke tool het antwoord genereerde.

Internationaal handhaven rechtbanken dit steeds strenger. Eind 2025 waren er wereldwijd meer dan 700 gedocumenteerde gevallen van AI-gehallucineerde inhoud in juridische procedures. Sancties variëren van $2.000 tot meer dan $31.000 per incident. Alleen al in augustus 2025 sanctioneerden drie afzonderlijke Amerikaanse federale rechtbanken advocaten voor het indienen van door AI gefabriceerde citaten.

## De verificatiestack: drie verdedigingen die werken

Hallucinaties kunnen niet geëlimineerd worden. Maar ze kunnen worden onderschept. Drie architecturale verdedigingslagen, gecombineerd, verlagen het risico van systemisch naar beheersbaar:

| Verdediging | Wat het doet | Wat het onderschept |
|-------------|-------------|---------------------|
| **Brongestuurde retrieval** | Doorzoekt een gecureerd juridisch [corpus](/nl/glossary/corpus/) vóór het genereren | Voorkomt dat het model feiten verzint die het nooit heeft opgehaald |
| **Citatievalidatie** | Controleert elke geciteerde bron tegen het daadwerkelijke corpus | Onderschept gefabriceerde referenties en foutief toegeschreven inhoud |
| **Betrouwbaarheidsscore** | Signaleert onzekerheid expliciet bij elke claim | Markeert dun bewijs voordat u erop vertrouwt |

Geen enkele laag is op zichzelf voldoende. Brongestuurde retrieval hallucineert nog steeds — Stanford bewees dat. Citatievalidatie onderschept verzonnen referenties maar niet subtiele misinterpretaties. Betrouwbaarheidsscoring markeert onzekerheid maar vereist kalibratie.

De combinatie is wat telt. Elke laag vangt op wat de andere missen.

> De kost van valse zekerheid is altijd hoger dan de kost van eerlijke onzekerheid.

---

## Gerelateerde artikelen

- [Ik vroeg ChatGPT en Auryth dezelfde Belgische fiscale vragen — dit is wat er gebeurde](/nl/blog/chatgpt-vs-auryth-vergelijking-nl/)
- [Wat is RAG — en waarom het ertoe doet voor fiscalisten](/nl/blog/wat-is-rag-nl/)
- [Wat is confidence scoring — en waarom is het eerlijker dan een zelfzeker antwoord?](/nl/blog/confidence-scoring-uitgelegd-nl/)

---

## Hoe Auryth TX dit toepast

Auryth TX is gebouwd op de aanname dat hallucinaties onvermijdelijk zijn — en ontwerpt eromheen in plaats van te doen alsof ze niet bestaan.

Elk antwoord passeert een drielagige verificatiepijplijn: retrieval uit het gecureerde Belgische juridische corpus (niet het open internet), post-generatie citatievalidatie die elke gerefereerde bepaling controleert tegen de werkelijke tekst, en per-claim betrouwbaarheidsscoring die expliciet markeert wanneer het bewijs dun is.

Wanneer het systeem onvoldoende bronnen vindt, vertelt het u dat. Wanneer bronnen tegenstrijdig zijn, toont het beide kanten. Wanneer een geciteerde bepaling is gewijzigd sinds de relevante aanslagdatum, markeert het de temporele afwijking.

Het doel is niet om altijd gelijk te hebben. Het is om u altijd te vertellen hoeveel vertrouwen u in het antwoord kunt stellen.

[Ontdek hoe Auryth TX verificatie aanpakt →](/nl/waitlist/)

---

*Bronnen:*
*1. Dahl, M. et al. (2024). "[Large Legal Fictions: Profiling Legal Hallucinations in Large Language Models](https://academic.oup.com/jla/article/16/1/64/7699227)." Journal of Legal Analysis, 16(1), 64–93.*
*2. Magesh, V. et al. (2025). "[Hallucination-Free? Assessing the Reliability of Leading AI Legal Research Tools](https://arxiv.org/abs/2405.20362)." Journal of Empirical Legal Studies.*
*3. Xu, Z., Jain, S. & Kankanhalli, M. (2024). "[Hallucination is Inevitable: An Innate Limitation of Large Language Models](https://arxiv.org/abs/2401.11817)." arXiv:2401.11817.*
*4. Kalai, A.T., Nachum, O., Vempala, S.S. & Zhang, E. (2025). "[Why Language Models Hallucinate](https://arxiv.org/abs/2509.04664)." OpenAI.*
