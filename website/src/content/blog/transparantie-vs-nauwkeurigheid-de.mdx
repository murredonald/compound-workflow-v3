---
title: "Warum Transparenz wichtiger ist als Genauigkeit bei juristischer KI"
description: "Die KI-Branche ist von Genauigkeits-Benchmarks besessen. Für Steuerberater ist Verifizierbarkeit die Kennzahl, die Sie wirklich schützt."
publishDate: 2026-02-14
author: "Auryth Team"
category: "trust-transparency"
category_name: "Vertrauen & Transparenz"
tags: ["Transparenz", "KI-Zuverlässigkeit", "juristische KI", "Verifizierbarkeit", "Berufshaftung"]
locale: "de"
draft: false
---

Eine KI mit 95% Genauigkeit, die nie sagt „ich bin nicht sicher", ist gefährlicher als eine mit 90% Genauigkeit, die immer ihre Quellen zeigt.

Das klingt widersprüchlich. Die KI-Branche hat uns jahrelang trainiert, eine einzige Frage zu stellen: *Wie genau ist Ihr Tool?* Höhere Zahl, besseres Tool, Ende der Diskussion. Aber wenn Sie ein Steuerberater sind, der die volle Berufshaftung für jede Beratung trägt, ist Genauigkeit ohne Transparenz eine Falle, die Sie erst sehen, wenn es zu spät ist.

## Die falsche Kennzahl

Genauigkeit ist eine Eigenschaft des Systems. Verifizierbarkeit ist eine Eigenschaft der Ausgabe. Das ist nicht dasselbe.

Wenn ein KI-Tool Ihnen mitteilt, dass der TOB-Satz auf thesaurierende Fonds 1,32% beträgt, bedeutet Genauigkeit, dass die Antwort korrekt ist. Verifizierbarkeit bedeutet, dass Sie sehen können, dass die Antwort auf eine bestimmte gesetzliche Bestimmung zurückgeht, die Quelle selbst einsehen und bestätigen können, dass sie das aussagt, was das Tool behauptet.

Eine genaue Antwort, die Sie nicht verifizieren können, ist eine Meinung einer Autorität, die Sie nicht hinterfragen können. Eine verifizierbare Antwort — selbst wenn sie gelegentlich falsch ist — ist ein Ausgangspunkt für professionelles Urteil. Sie können den Fehler erkennen. Sie können die Argumentation bewerten. Sie können Ihre Entscheidung verteidigen.

Die KI-Branche misst dies nicht. Anbieter veröffentlichen Genauigkeits-Benchmarks, keine Verifizierbarkeits-Scores. Denn Genauigkeit schmeichelt und Verifizierbarkeit fordert.

## Warum KI sicherer klingt als sie sollte

Moderne KI hat einen Designfehler, der Genauigkeitskennzahlen besonders irreführend macht: Sie ist *darauf trainiert, sicher zu klingen*.

Große Sprachmodelle durchlaufen **[Reinforcement Learning from Human Feedback](/de/glossary/rlhf/)** (RLHF), bei dem menschliche Bewerter sichere, gut strukturierte Antworten belohnen und Differenzierungen bestrafen. Das Ergebnis: KI-Systeme sind architektonisch zur Sicherheit verzerrt — selbst wenn die zugrunde liegenden Belege dünn oder nicht vorhanden sind.

Stanford-Forscher fanden heraus, dass selbst Premium-[RAG](/de/glossary/rag/)-Tools für juristische Recherche bei 17–33% der Anfragen halluzinieren. Allgemeine [LLMs](/de/glossary/llm/) schneiden schlechter ab: 58–88% Fehlerquote bei juristischen Fragen. Aber das Gefährliche ist nicht die Fehlerquote selbst — es ist, dass die Fehler nicht *anders aussehen* als die korrekten Antworten. Ein fabriziertes Zitat liest sich genau wie ein echtes. Eine falsche Artikelnummer wird mit derselben Sicherheit angegeben wie die richtige.

Ohne Quellentransparenz können Sie die 67%, die korrekt sind, nicht von den 33% unterscheiden, die es nicht sind.

![Die Falschsicherheitssteuer: jeder Prozentpunkt KI-Genauigkeit, den Sie nicht verifizieren können, ist eine Haftung, die Sie nicht managen können](/blog/transparantie-vs-nauwkeurigheid/false-certainty-tax-quote-de-dark.png)

## Wie Transparenz in der Praxis aussieht

Transparenz bei juristischer KI ist kein Marketing-Häkchen. Es ist eine Reihe spezifischer technischer Entscheidungen, die verändern, wie Sie das Tool professionell einsetzen können:

![Der Kompromiss zwischen Genauigkeit und Verifizierbarkeit: wo verschiedene KI-Ansätze stehen](/blog/transparantie-vs-nauwkeurigheid/accuracy-verifiability-spectrum-de-dark.png)

| Fähigkeit | Undurchsichtige KI | Transparente KI |
|-----------|-------------------|-----------------|
| **Quellenangaben** | „Basierend auf belgischem Steuerrecht" | „Art. 19bis EStGB 92, geändert am 25. Dezember 2017" mit einsehbarer Quelle |
| **Vertrauenssignal** | Jede Antwort mit gleicher Sicherheit | Konfidenz-Score pro Behauptung basierend auf Quellenabdeckung und Übereinstimmung |
| **Dünne Beweislage** | Antwortet trotzdem, volles Vertrauen | Signalisiert explizit: „Keine spezifische Entscheidung zu diesem Punkt gefunden" |
| **Widersprüchliche Quellen** | Wählt eine, präsentiert als definitiv | Zeigt beide Seiten nach Rechtsautorität geordnet |
| **Prüfpfad** | Chat-Verlauf | Strukturiertes Protokoll: Anfrage, abgerufene Quellen, verworfene Quellen, Konfidenz-Begründung |
| **Unsicherheit** | Nie ausgedrückt | Explizit quantifiziert — niedrige Konfidenz löst eine Überprüfungswarnung aus |

Der Unterschied ist nicht kosmetisch. Es ist der Unterschied zwischen einem Tool, das Ihr Urteil ersetzt, und einem, das es informiert.

## Der Verifikationsstapel: drei Schichten des Vertrauens

Nicht alle Transparenz ist gleich. Eine [Quellenangabe](/de/glossary/citation/), die Sie nicht unabhängig prüfen können, ist Theater, keine Transparenz. Professionelle juristische KI braucht drei Schichten:

| Schicht | Welche Frage sie beantwortet | Was sie erfordert |
|---------|----------------------------|-------------------|
| **1. Zitattransparenz** | Kann ich die Quelle sehen? | Jede Behauptung verknüpft mit einem abrufbaren Dokument |
| **2. Zitatvalidierung** | Sagt die Quelle das wirklich? | Unabhängige NLI-Prüfung, die die Behauptung des Modells mit dem Quelltext vergleicht |
| **3. Autoritätskontext** | Wie stark ist diese Quelle? | Rechtshierarchie-Ranking — ein Urteil des Hof van Cassatie wiegt schwerer als ein Rundschreiben |

Entfernen Sie eine Schicht und der Transparenzanspruch bricht zusammen. Eine Quellenangabe ohne Validierung zu zeigen ist Zitatdekoration — es sieht glaubwürdig aus, ohne etwas zu beweisen. Zitate ohne Autoritätskontext zu validieren behandelt einen Blogbeitrag und ein Urteil des Obersten Gerichtshofs als gleichwertige Beweise.

> Der Verifikationsstapel: Zitattransparenz ohne Validierung ist Dekoration. Validierung ohne Autoritätskontext ist Demokratie. Sie brauchen alle drei.

## Warum dies für die belgische Berufshaftung relevant ist

Belgische Steuerberater und Buchhalter — ob beim ITAA registriert oder selbständig tätig — tragen die volle Berufshaftung für die Beratung, die sie ihren Mandanten erteilen. Kein KI-Tool ändert diese grundlegende Verpflichtung. Ein Tool zu nutzen, das fehlerhafte Ergebnisse produziert hat, ist keine Verteidigung; sich auf ein nicht verifizierbares Tool ohne unabhängige Überprüfung zu verlassen, könnte unzureichende Sorgfalt darstellen.

Die EU-KI-Verordnung ([AI Act](/de/glossary/eu-ai-act/)), mit Transparenz- und Aufsichtsanforderungen, die schrittweise 2025–2026 in Kraft treten, fügt eine regulatorische Dimension hinzu. Für Berufstätige, die KI in der Beratungsarbeit einsetzen, ist die praktische Implikation einfach: Die Tools, die Sie verwenden, müssen Ihre Fähigkeit unterstützen, deren Ergebnisse zu verifizieren und zu beaufsichtigen. Ein undurchsichtiges System erschwert die Compliance.

Eine aktuelle Thomson-Reuters-Umfrage ergab, dass 59% der Unternehmensjuristen nicht einmal wissen, ob ihre externen Berater generative KI nutzen. Diese Informationsasymmetrie — KI-gestützte Beratung ohne Offenlegung — ist genau die Art von Undurchsichtigkeit, die der regulatorische Trend beseitigen will.

Die praktische Frage für jeden Steuerberater, der ein KI-Recherchetool evaluiert, lautet nicht „Wie genau ist das?" Sondern: **Kann ich die Ergebnisse schnell genug verifizieren, damit mir das Tool tatsächlich Zeit spart?**

## Das Gegenargument: Zählt Genauigkeit denn nicht?

Natürlich. Niemand will ein transparentes Tool, das die Hälfte der Zeit falsch liegt. Aber die Debatte stellt nicht Genauigkeit *gegen* Transparenz — es geht darum zu erkennen, dass Transparenz Genauigkeit erst *nutzbar* macht.

Ein Tool mit 95% Genauigkeit ohne Verifizierungspfad bedeutet, dass Sie jede Antwort unabhängig recherchieren müssen, um die 5% Fehler zu finden — was die Zeitersparnis zunichtemacht. Ein Tool mit 90% Genauigkeit, das seine Quellen zeigt, Unsicherheit signalisiert und nach Autorität ordnet, bedeutet, dass Sie die 90% schnell bestätigen und Ihre Expertise auf die 10% konzentrieren können.

Die Ironie: Transparente Tools *wirken* weniger beeindruckend, weil sie ihre Unsicherheit zeigen. Undurchsichtige Tools machen in Demos mehr Eindruck, weil sie bei jeder Antwort Sicherheit ausstrahlen. Aber in der professionellen Praxis ist das Tool, das sagt „Ich habe begrenzte Autorität zu diesem Punkt gefunden — hier sind zwei widersprüchliche Quellen, nach Gewicht geordnet", unendlich nützlicher als eines, das sagt „Die Antwort ist X", ohne Überprüfungsmöglichkeit.

> Die gefährlichste KI ist nicht die, die manchmal falsch liegt. Es ist die, die Ihnen nie sagt, wann das der Fall ist.

---

## Verwandte Artikel

- [Was ist RAG — und warum es allein nicht für juristische KI ausreicht](/de/blog/wat-is-rag-de/)
- [KI-Halluzinationen: warum ChatGPT Quellen erfindet (und wie Sie das erkennen)](/de/blog/ai-hallucinaties-fiscaal-de/)
- [Confidence Scoring: warum es ehrlicher ist als eine selbstsichere Antwort](/de/blog/confidence-scoring-uitgelegd-de/)

---

## Wie Auryth TX das umsetzt

Auryth TX ist auf dem Verifizierbarkeits-Prinzip gebaut — der Überzeugung, dass ein transparentes Tool sicherer ist als ein undurchsichtiges, unabhängig von Genauigkeits-Benchmarks.

Jede Antwort enthält Quellenangaben pro Behauptung, verknüpft mit abrufbaren Dokumenten im belgischen Rechtscorpus. Jedes Zitat wird unabhängig validiert: Eine Natural-Language-[Inference](/de/glossary/inference/)-Schicht prüft, ob jede Quelle tatsächlich das aussagt, was das Modell ihr zuschreibt. Jede Quelle trägt eine Autoritätsstufe — von Verfassungsbestimmungen über Gesetzgebung, Rechtsprechung, Rundschreiben bis zur Doktrin — damit Sie das Gewicht der Beweise kennen, nicht nur deren Existenz.

Wenn Beweise dünn sind, sinkt der Konfidenz-Score und erklärt warum. Wenn Quellen sich widersprechen, werden beide Seiten mit ihrem Autoritätsranking gezeigt. Wenn das System sucht und nichts Relevantes findet, teilt es Ihnen das mit — denn zu wissen, dass keine Autorität zu einem bestimmten Punkt existiert, ist professionelle Intelligenz, kein Systemversagen.

Wir veröffentlichen unsere Genauigkeitskennzahlen. Wir zeigen unsere Quellen. Wir signalisieren unsere Unsicherheit. Nicht weil es uns besser aussehen lässt — das tut es nicht. Sondern weil Vertrauen ohne Beweise Marketing ist, und wir ein Recherchetool bauen.

[Entdecken Sie unseren Ansatz zur Transparenz — Warteliste beitreten →](/de/waitlist/)

---

*Quellen:*
*1. Magesh, V. et al. (2025). „[Hallucination-Free? Assessing the Reliability of Leading AI Legal Research Tools](https://arxiv.org/abs/2405.20362)." Journal of Empirical Legal Studies.*
*2. Thomson Reuters Institute (2024). „ChatGPT & Generative AI within Law Firms."*
*3. Europäisches Parlament (2024). „[Verordnung (EU) 2024/1689 — Die EU-KI-Verordnung](https://eur-lex.europa.eu/eli/reg/2024/1689/oj/eng)."*
