---
term: "Byte Pair Encoding (BPE)"
termSlug: "byte-pair-encoding"
short: "A subword tokenization algorithm that builds a vocabulary by iteratively merging frequent symbol pairs."
category: "ai-ml"
category_name: "AI & Machine Learning"
related: ["tokenization", "sentencepiece", "llm"]
synonyms: ["BPE", "Byte-pair tokenization"]
locale: "en"
draft: false
---

## Definition

Byte Pair Encoding (BPE) is a data-driven [tokenization](/en/glossary/tokenization/) method that starts from individual characters and repeatedly merges the most frequent adjacent symbol pairs to create a compact subword vocabulary for language models.

## References

- Sennrich et al. (2016), "[Neural Machine Translation of Rare Words with Subword Units](https://doi.org/10.18653/v1/P16-1162)", ACL.

- Gage (1994), "[A New Algorithm for Data Compression](https://www.derczynski.com/papers/archive/BPE_Gage.pdf)", C Users Journal.

- Kudo & Richardson (2018), "[SentencePiece: A simple and language independent subword tokenizer and detokenizer](https://doi.org/10.18653/v1/D18-2012)", EMNLP.
