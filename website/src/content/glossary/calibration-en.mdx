---
term: "Calibration"
termSlug: "calibration"
short: "Aligning model confidence scores with the true likelihood of correctness."
category: "ai-ml"
category_name: "AI & Machine Learning"
related: ["uncertainty-estimation", "confidence-interval", "reliability-metrics"]
synonyms: ["Probability calibration", "Score calibration"]
locale: "en"
draft: false
---

## Definition

Calibration adjusts or evaluates how well predicted probabilities match observed frequenciesâ€”for example, whether answers given with 80% confidence are correct about 80% of the time.

## References

- Guo et al. (2017), "[On Calibration of Modern Neural Networks](https://arxiv.org/abs/1706.04599)", ICML.

- Naeini et al. (2015), "[Obtaining Well Calibrated Probabilities Using Bayesian Binning](https://doi.org/10.1609/aaai.v29i1.9602)", AAAI.

- Minderer et al. (2021), "[Revisiting the Calibration of Modern Neural Networks](https://arxiv.org/abs/2106.07998)", NeurIPS.
