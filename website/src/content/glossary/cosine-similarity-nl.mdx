---
term: "Cosinus-gelijkenis"
termSlug: "cosine-similarity"
short: "Een wiskundige maat voor gelijkenis tussen twee vectoren gebaseerd op de cosinus van de hoek ertussen."
category: "ai-ml"
category_name: "AI & Machine Learning"
related: ["embeddings", "semantic-similarity", "vector-database", "semantic-search"]
synonyms: ["Cosinus-afstand", "Hoekgelijkenis", "Vectorgelijkenis"]
locale: "nl"
draft: false
---

## Definitie

Cosinus-gelijkenis meet hoe vergelijkbaar twee vectoren zijn door de cosinus te berekenen van de hoek ertussen. Waarden variëren van -1 (tegengesteld) tot 1 (identiek), met 0 als orthogonaal (geen gelijkenis). Het is de meest gebruikte metriek voor het vergelijken van tekstembeddings omdat het focust op richting (betekenis) in plaats van magnitude (lengte).

## Waarom het belangrijk is

Cosinus-gelijkenis is fundamenteel voor moderne AI-zoekopdrachten en retrieval:

- **Richting boven magnitude** — vangt semantische oriëntatie op, niet vectorlengte
- **Genormaliseerde vergelijking** — werkt met [embeddings](/nl/glossary/vector-embeddings/) van verschillende magnitudes
- **Efficiëntie** — snel te berekenen, vooral met geoptimaliseerde libraries
- **[Interpreteerbaarheid](/nl/glossary/explainability/)** — makkelijk te begrijpen: 1 = zelfde, 0 = niet gerelateerd, -1 = tegengesteld
- **Standaardmetriek** — default in de meeste vectordatabases en embedding APIs

Het maakt betekenisvolle vergelijking mogelijk van tekst, afbeeldingen en andere embedded representaties.

## Hoe het werkt

```
┌────────────────────────────────────────────────────────────┐
│                   COSINUS-GELIJKENIS                       │
├────────────────────────────────────────────────────────────┤
│                                                            │
│         FORMULE: cos(θ) = (A · B) / (||A|| × ||B||)        │
│                                                            │
│  ┌─────────────────────────────────────────────────────┐   │
│  │                                                     │   │
│  │              B                                      │   │
│  │             /                                       │   │
│  │            /  θ = hoek                              │   │
│  │           /                                         │   │
│  │          /                                          │   │
│  │         ──────────────► A                           │   │
│  │                                                     │   │
│  │  cos(0°) = 1.0    → Identieke richting              │   │
│  │  cos(45°) ≈ 0.71  → Vergelijkbare richting          │   │
│  │  cos(90°) = 0.0   → Orthogonaal (niet gerelateerd)  │   │
│  │  cos(180°) = -1.0 → Tegengestelde richting          │   │
│  │                                                     │   │
│  └─────────────────────────────────────────────────────┘   │
│                                                            │
│  VOORBEELDBEREKENING:                                      │
│  Vector A = [0.8, 0.6]                                     │
│  Vector B = [0.6, 0.8]                                     │
│                                                            │
│  A · B = (0.8 × 0.6) + (0.6 × 0.8) = 0.96                  │
│  ||A|| = √(0.8² + 0.6²) = 1.0                              │
│  ||B|| = √(0.6² + 0.8²) = 1.0                              │
│                                                            │
│  cos(θ) = 0.96 / (1.0 × 1.0) = 0.96 → Zeer vergelijkbaar  │
│                                                            │
└────────────────────────────────────────────────────────────┘
```

**Vergelijking met andere metrieken:**
| Metriek | Formule | Wanneer gebruiken |
|---------|---------|-------------------|
| Cosinus | 1 - cos(θ) | Genormaliseerde embeddings, tekstgelijkenis |
| Euclidisch | √Σ(a-b)² | Absolute afstanden belangrijk |
| Dotproduct | Σ(a×b) | Wanneer vectoren al genormaliseerd zijn |

## Veelgestelde vragen

**V: Waarom cosinus-gelijkenis in plaats van [Euclidische afstand](/nl/glossary/euclidean-distance/)?**

A: Cosinus negeert vectormagnitude en focust alleen op richting. Twee documenten over "belastingrecht" zouden vergelijkbaar moeten zijn, ook als het ene langer is met grotere embeddingmagnitude. Cosinus vangt dit op; Euclidisch behandelt ze als meer verschillend.

**V: Wat betekent een cosinus-gelijkenis van 0.8?**

A: De vectoren wijzen in bijna dezelfde richting—ze zijn semantisch vergelijkbaar. Voor tekstembeddings duidt 0.8+ typisch op sterke relevantie. Drempels variëren echter per model; kalibreer met je data.

**V: Kan cosinus-gelijkenis negatief zijn?**

A: Ja, wanneer vectoren in tegengestelde richtingen wijzen. Met de meeste tekstembeddings zijn negatieven zeldzaam omdat embeddingmodellen typisch vectoren in positieve ruimte produceren. Een waarde nabij 0 is gebruikelijker voor niet-gerelateerde content.

**V: Is cosinus-gelijkenis hetzelfde als cosinus-afstand?**

A: Ze zijn omgekeerden. Cosinus-afstand = 1 - cosinus-gelijkenis. Databases gebruiken vaak "afstand" (lager = meer vergelijkbaar) terwijl APIs "gelijkenis" rapporteren (hoger = meer vergelijkbaar). Controleer de conventie van je tool.

## Gerelateerde termen

- [Embeddings](/nl/glossary/embeddings/) — vectoren die cosinus-gelijkenis vergelijkt
- [Semantische Gelijkenis](/nl/glossary/semantic-similarity/) — concept gemeten door cosinus-gelijkenis
- [Vector Database](/nl/glossary/vector-database/) — gebruikt cosinus voor [nearest-neighbor search](/nl/glossary/nearest-neighbor-search/)
- [Semantic Search](/nl/glossary/semantic-search/) — retrieval aangedreven door cosinusvergelijkingen

---

## Referenties

> Singhal (2001), "[Modern Information Retrieval: A Brief Overview](http://singhal.info/ieee2001.pdf)", IEEE Data Engineering Bulletin. [3.000+ [citaties](/nl/glossary/citation/)]

> Manning et al. (2008), "[Introduction to Information Retrieval](https://nlp.stanford.edu/IR-book/)", Cambridge University Press. [20.000+ citaties]

> Mikolov et al. (2013), "[Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/abs/1301.3781)", arXiv. [35.000+ citaties]

> Johnson et al. (2019), "[Billion-scale similarity search with GPUs](https://arxiv.org/abs/1702.08734)", IEEE Transactions on Big Data. [1.500+ citaties]

## References

> Singhal (2001), "[Modern Information Retrieval: A Brief Overview](http://singhal.info/ieee2001.pdf)", IEEE Data Engineering Bulletin. [3,000+ citations]

> Manning et al. (2008), "[Introduction to Information Retrieval](https://nlp.stanford.edu/IR-book/)", Cambridge University Press. [20,000+ citations]

> Mikolov et al. (2013), "[Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/abs/1301.3781)", arXiv. [35,000+ citations]

> Johnson et al. (2019), "[Billion-scale similarity search with GPUs](https://arxiv.org/abs/1702.08734)", IEEE Transactions on Big Data. [1,500+ citations]
