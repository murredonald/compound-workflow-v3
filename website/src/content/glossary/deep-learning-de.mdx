---
term: "Deep Learning"
termSlug: "deep-learning"
short: "Ein Teilbereich des Machine Learning, der neuronale Netze mit vielen Schichten nutzt, um hierarchische ReprÃ¤sentationen zu lernen."
category: "ai-ml"
category_name: "KI & Machine Learning"
related: ["neural-network", "transformer-architecture", "backpropagation", "llm"]
synonyms: ["Tiefe neuronale Netze", "DNN", "Hierarchisches Lernen", "ReprÃ¤sentationslernen"]
locale: "de"
draft: false
---

## Definition

Deep Learning ist ein Zweig des maschinellen Lernens, der kÃ¼nstliche neuronale Netze mit mehreren Schichten (daher "tief") verwendet, um automatisch hierarchische ReprÃ¤sentationen von Daten zu lernen. Im Gegensatz zu flachen Modellen, die handgefertigte Features erfordern, lernen Deep-Learning-Systeme auf jeder Schicht zunehmend abstrakte Featuresâ€”von Kanten und Texturen in Bildern bis zu semantischen Konzepten, oder von Zeichen zu WÃ¶rtern zu SÃ¤tzen zu Bedeutung in Text.

## Warum es wichtig ist

Deep Learning revolutionierte KI:

- **Automatische Feature-Extraktion** â€” kein manuelles Feature Engineering nÃ¶tig
- **Hierarchische Abstraktion** â€” lernt Konzepte auf mehreren Ebenen
- **Skalierbare Leistung** â€” verbessert sich mit mehr Daten und Rechenleistung
- **Transfer Learning** â€” vortrainierte Modelle passen sich neuen Aufgaben an
- **Durchbruch-Ergebnisse** â€” treibt Bilderkennung, NLP, AlphaGo, LLMs an

Jeder groÃŸe KI-Fortschritt seit 2012 wurde durch Deep Learning angetrieben.

## Wie es funktioniert

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      DEEP LEARNING                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                            â”‚
â”‚  FLACHE VS TIEFE ARCHITEKTUR:                              â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                              â”‚
â”‚                                                            â”‚
â”‚  FLACH (1-2 Schichten):         TIEF (viele Schichten):   â”‚
â”‚                                                            â”‚
â”‚  Eingabe â”€â”€â–º Hidden â”€â”€â–º Ausgabe Eingabe                   â”‚
â”‚                                   â”‚                        â”‚
â”‚                                   â–¼                        â”‚
â”‚                                 Schicht 1 (niedrig)        â”‚
â”‚                                   â”‚                        â”‚
â”‚                                   â–¼                        â”‚
â”‚                                 Schicht 2                  â”‚
â”‚                                   â”‚                        â”‚
â”‚                                   â–¼                        â”‚
â”‚                                 Schicht 3                  â”‚
â”‚                                   â”‚                        â”‚
â”‚                                   â–¼                        â”‚
â”‚                                 ...                        â”‚
â”‚                                   â”‚                        â”‚
â”‚                                   â–¼                        â”‚
â”‚                                 Schicht N (hoch)           â”‚
â”‚                                   â”‚                        â”‚
â”‚                                   â–¼                        â”‚
â”‚                                 Ausgabe                    â”‚
â”‚                                                            â”‚
â”‚  HIERARCHISCHES FEATURE-LERNEN (Bild-Beispiel):            â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€            â”‚
â”‚                                                            â”‚
â”‚  Schicht 1: â”Œâ”€â”€â”€â” â”Œâ”€â”€â”€â” â”Œâ”€â”€â”€â”                             â”‚
â”‚  (Kanten)   â”‚ / â”‚ â”‚ â”€ â”‚ â”‚ \ â”‚   Erkennt Kanten           â”‚
â”‚             â””â”€â”€â”€â”˜ â””â”€â”€â”€â”˜ â””â”€â”€â”€â”˜                             â”‚
â”‚                   â”‚                                        â”‚
â”‚                   â–¼                                        â”‚
â”‚  Schicht 2: â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”                               â”‚
â”‚  (Formen)   â”‚  â—‹  â”‚ â”‚ â–¡â”€â”€ â”‚   Kombiniert zu Formen       â”‚
â”‚             â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜                               â”‚
â”‚                   â”‚                                        â”‚
â”‚                   â–¼                                        â”‚
â”‚  Schicht 3: â”Œâ”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”                           â”‚
â”‚  (Teile)    â”‚ (â—•â€¿â—•) â”‚ â”‚  ðŸ¦»   â”‚   Bildet Objektteile     â”‚
â”‚             â””â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”˜                           â”‚
â”‚                   â”‚                                        â”‚
â”‚                   â–¼                                        â”‚
â”‚  Schicht N: â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                           â”‚
â”‚  (Objekt)   â”‚    "KATZE"      â”‚   Erkennt Objekte        â”‚
â”‚             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                           â”‚
â”‚                                                            â”‚
â”‚  DEEP LEARNING ARCHITEKTUREN:                              â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                              â”‚
â”‚  CNNs:        Bilder, rÃ¤umliche Muster                    â”‚
â”‚  RNNs/LSTMs:  Sequenzen, Zeitreihen                       â”‚
â”‚  Transformers: Sprache, Vision (heute dominant)           â”‚
â”‚  GANs:        Generative Aufgaben                         â”‚
â”‚  Autoencoders: Kompression, Entrauschen                   â”‚
â”‚                                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Warum Tiefe wichtig ist:**
| Aspekt | Flaches Netzwerk | Tiefes Netzwerk |
|--------|------------------|-----------------|
| Feature-Lernen | Manuell oder begrenzt | Automatisch, hierarchisch |
| Abstraktion | Einzelne Ebene | Mehrere Ebenen |
| AusdrucksstÃ¤rke | Begrenzte KomplexitÃ¤t | Sehr komplexe Funktionen |
| Dateneffizienz | Mehr Daten pro Feature | Lernt wiederverwendbare Features |

## HÃ¤ufige Fragen

**F: Wie viele Schichten machen ein Netzwerk "tief"?**

A: Generell gelten 3+ versteckte Schichten als "tief", obwohl moderne LLMs 32-100+ Schichten haben. Der Begriff ist relativâ€”was 2010 "tief" war (5-8 Schichten) ist heute flach. Tiefe bezieht sich auf das Lernen hierarchischer ReprÃ¤sentationen, nicht auf eine feste Zahl.

**F: Warum startete Deep Learning 2012 durch?**

A: Drei Faktoren kamen zusammen: (1) GPUs ermÃ¶glichten Training groÃŸer Netzwerke, (2) groÃŸe DatensÃ¤tze wie ImageNet wurden verfÃ¼gbar, (3) algorithmische Verbesserungen wie ReLU-Aktivierung und Dropout verbesserten das Training. AlexNets ImageNet-Sieg demonstrierte das Potenzial.

**F: Was ist die Beziehung zwischen Deep Learning und KI?**

A: Deep Learning ist eine Untermenge von [Machine Learning](/de/glossary/machine-learning/), das eine Untermenge von KI ist. Nicht alle KI nutzt Deep Learning (regelbasierte Systeme nicht), und nicht alles Machine Learning ist tief (EntscheidungsbÃ¤ume, SVMs nicht). Aber Deep Learning treibt jetzt die meisten hochmodernen KI-Systeme an.

**F: Kann Deep Learning jedes Problem lÃ¶sen?**

A: Nein. Deep Learning excelliert bei Mustererkennung mit vielen Daten, hat aber Schwierigkeiten mit: kleinen DatensÃ¤tzen, Schlussfolgerung, kausaler [Inferenz](/de/glossary/inference/), Extrapolation Ã¼ber Trainingsdaten hinaus, und Aufgaben, die explizite symbolische Logik erfordern. Es ist ein mÃ¤chtiges Werkzeug, keine universelle LÃ¶sung.

## Verwandte Begriffe

- [Neuronales Netz](/de/glossary/neural-network/) â€” das Fundament von Deep Learning
- [Transformer-Architektur](/de/glossary/transformer-architecture/) â€” dominante tiefe Architektur
- [Backpropagation](/de/glossary/backpropagation/) â€” Algorithmus, der Deep Learning ermÃ¶glicht
- [LLM](/de/glossary/llm/) â€” groÃŸskaliges Deep Learning fÃ¼r Sprache

---

## Referenzen

> LeCun et al. (2015), "[Deep Learning](https://www.nature.com/articles/nature14539)", Nature. [40.000+ Zitationen]

> Goodfellow et al. (2016), "[Deep Learning](https://www.deeplearningbook.org/)", MIT Press. [Umfassendes Lehrbuch]

> Krizhevsky et al. (2012), "[ImageNet Classification with Deep CNNs](https://papers.nips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html)", NeurIPS. [AlexNet - lÃ¶ste Deep Learning Revolution aus]

> Bengio et al. (2013), "[Representation Learning: A Review](https://arxiv.org/abs/1206.5538)", IEEE TPAMI. [15.000+ Zitationen]

## References

> LeCun et al. (2015), "[Deep Learning](https://www.nature.com/articles/nature14539)", Nature. [40,000+ citations]

> Goodfellow et al. (2016), "[Deep Learning](https://www.deeplearningbook.org/)", MIT Press. [Comprehensive textbook]

> Krizhevsky et al. (2012), "[ImageNet Classification with Deep CNNs](https://papers.nips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html)", NeurIPS. [AlexNet - sparked deep learning revolution]

> Bengio et al. (2013), "[Representation Learning: A Review](https://arxiv.org/abs/1206.5538)", IEEE TPAMI. [15,000+ citations]
