---
term: "Evals framework"
termSlug: "evals-framework"
short: "A reusable setup for defining, running, and tracking evaluations of AI systems."
category: "ai-ml"
category_name: "AI & Machine Learning"
related: ["evaluation-dataset", "benchmarking", "continuous-evaluation"]
synonyms: ["Evaluation framework", "Evals harness"]
locale: "en"
draft: false
---

## Definition

An evals framework provides tooling and configuration to run repeatable tests—on [prompts](/en/glossary/prompt/), models, or pipelines—and to store [metrics](/en/glossary/distance-metric/) and results over time.

## References

> K. Singhal et al. (2022), "[Large language models encode clinical knowledge](https://arxiv.org/abs/2212.13138)", Nature.

> Jiawei Liu et al. (2023), "[Is Your Code Generated by ChatGPT Really Correct? Rigorous Evaluation of Large Language Models for Code Generation](https://arxiv.org/abs/2305.01210)", Neural Information Processing Systems.

> Yunfan Gao et al. (2023), "[Retrieval-Augmented Generation for Large Language Models: A Survey](https://doi.org/10.48550/arxiv.2312.10997)", arXiv.
