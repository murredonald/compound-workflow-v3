---
term: "Framework d'évaluation"
termSlug: "evals-framework"
short: "Dispositif réutilisable pour définir, exécuter et suivre des scénarios d'évaluation d'IA."
category: "ai-ml"
category_name: "IA & Machine Learning"
related: ["evaluation-dataset", "benchmarking", "continuous-evaluation"]
synonyms: ["Cadre d'évaluation"]
locale: "fr"
draft: false
---

## Définition

Un framework d'évaluation fournit les outils et configurations pour lancer et tracer des tests reproductibles sur modèles et pipelines.

## References

> K. Singhal et al. (2022), "[Large language models encode clinical knowledge](https://arxiv.org/abs/2212.13138)", Nature.

> Jiawei Liu et al. (2023), "[Is Your Code Generated by ChatGPT Really Correct? Rigorous Evaluation of Large Language Models for Code Generation](https://arxiv.org/abs/2305.01210)", Neural Information Processing Systems.

> Yunfan Gao et al. (2023), "[Retrieval-Augmented Generation for Large Language Models: A Survey](https://doi.org/10.48550/arxiv.2312.10997)", arXiv.
