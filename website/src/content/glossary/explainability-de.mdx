---
term: "ErklÃ¤rbarkeit"
termSlug: "explainability"
short: "Die FÃ¤higkeit zu verstehen, zu interpretieren und zu erklÃ¤ren, wie KI/ML-Modelle Vorhersagen treffenâ€”essentiell fÃ¼r Vertrauen, Debugging, regulatorische Compliance und verantwortungsvolle KI-Bereitstellung."
category: "ai-ml"
category_name: "KI & Machine Learning"
related: ["interpretability", "transparency", "black-box", "xai", "feature-importance", "ai-act"]
synonyms: ["Interpretierbarkeit", "XAI", "ErklÃ¤rbare KI", "Modelltransparenz"]
locale: "de"
draft: false
---

## Definition

ErklÃ¤rbarkeit (oder Explainable AI/XAI) bezeichnet Techniken, die KI-Modellentscheidungen fÃ¼r Menschen verstÃ¤ndlich machen. Sie beantwortet "warum hat das Modell diese [Vorhersage](/de/glossary/inference/) gemacht?" statt nur "was hat das Modell vorhergesagt?" [ErklÃ¤rbarkeit](/de/glossary/algorithmic-transparency/) existiert auf einem Spektrum: von inhÃ¤rent interpretierbaren Modellen (lineare Regression, EntscheidungsbÃ¤ume) bis zu Post-hoc-ErklÃ¤rungen fÃ¼r Black-Box-Modelle (SHAP, LIME, Attention-Visualisierung). Da KI-Systeme zunehmend folgenreiche Entscheidungen treffen (Gesundheit, Finanzen, Recht), wird ErklÃ¤rbarkeit entscheidend fÃ¼r Vertrauen, Verantwortlichkeit, Debugging und regulatorische Compliance ([AI Act](/de/glossary/eu-ai-act/), DSGVO Artikel 22).

## Warum es wichtig ist

ErklÃ¤rbarkeit adressiert kritische KI-Bereitstellungsanforderungen:

- **Vertrauen** â€” Nutzer vertrauen Systemen, die sie verstehen
- **Debugging** â€” identifizieren warum Modelle in spezifischen FÃ¤llen versagen
- **Regulatorische Compliance** â€” AI Act erfordert ErklÃ¤rungen fÃ¼r Hochrisiko-KI
- **Bias-Erkennung** â€” aufdecken ob Modelle geschÃ¼tzte Attribute verwenden
- **DomÃ¤nenvalidierung** â€” Experten verifizieren dass Modelllogik stimmig ist
- **Rechtliche VerteidigungsfÃ¤higkeit** â€” automatisierte Entscheidungen erklÃ¤ren

## Wie es funktioniert

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     ERKLÃ„RBARKEIT                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                            â”‚
â”‚  DAS ERKLÃ„RBARKEITS-SPEKTRUM:                              â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                              â”‚
â”‚                                                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚                                                      â”‚ â”‚
â”‚  â”‚    INTERPRETIERBAR         â†’           BLACK-BOX    â”‚ â”‚
â”‚  â”‚    (eingebaut)                       (braucht XAI)  â”‚ â”‚
â”‚  â”‚                                                      â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚
â”‚  â”‚  â”‚   LINEARE   â”‚  â”‚   RANDOM    â”‚  â”‚   TIEFES    â”‚ â”‚ â”‚
â”‚  â”‚  â”‚  REGRESSION â”‚  â”‚   FOREST    â”‚  â”‚  NEURONALES â”‚ â”‚ â”‚
â”‚  â”‚  â”‚             â”‚  â”‚             â”‚  â”‚   NETZWERK  â”‚ â”‚ â”‚
â”‚  â”‚  â”‚ Koeffizient â”‚  â”‚ Feature     â”‚  â”‚             â”‚ â”‚ â”‚
â”‚  â”‚  â”‚ = direkte   â”‚  â”‚ Importance  â”‚  â”‚ ğŸ¤· ???     â”‚ â”‚ â”‚
â”‚  â”‚  â”‚ ErklÃ¤rung   â”‚  â”‚ verfÃ¼gbar   â”‚  â”‚ Braucht SHAPâ”‚ â”‚ â”‚
â”‚  â”‚  â”‚             â”‚  â”‚             â”‚  â”‚ LIME, etc.  â”‚ â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚
â”‚  â”‚                                                      â”‚ â”‚
â”‚  â”‚  â—„â”€â”€â”€ Mehr interpretierbar   Weniger interpret. â”€â”€â–º â”‚ â”‚
â”‚  â”‚  â—„â”€â”€â”€ Weniger leistungsfÃ¤hig  LeistungsfÃ¤higer â”€â”€â–º  â”‚ â”‚
â”‚  â”‚                                                      â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                            â”‚
â”‚                                                            â”‚
â”‚  ARTEN VON ERKLÃ„RUNGEN:                                    â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                    â”‚
â”‚                                                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚                                                      â”‚ â”‚
â”‚  â”‚  1. GLOBALE ERKLÃ„RUNGEN                             â”‚ â”‚
â”‚  â”‚     "Wie funktioniert das Modell generell?"         â”‚ â”‚
â”‚  â”‚                                                      â”‚ â”‚
â”‚  â”‚     Feature Importance (gesamt)                     â”‚ â”‚
â”‚  â”‚     Alter:     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  (45%)             â”‚ â”‚
â”‚  â”‚     Einkommen: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ       (28%)             â”‚ â”‚
â”‚  â”‚     Standort:  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ             (15%)             â”‚ â”‚
â”‚  â”‚     Historie:  â–ˆâ–ˆâ–ˆâ–ˆ              (12%)             â”‚ â”‚
â”‚  â”‚                                                      â”‚ â”‚
â”‚  â”‚  2. LOKALE ERKLÃ„RUNGEN                              â”‚ â”‚
â”‚  â”‚     "Warum hat das Modell DIESE Vorhersage gemacht?"â”‚ â”‚
â”‚  â”‚                                                      â”‚ â”‚
â”‚  â”‚     Vorhersage: ABGELEHNT                           â”‚ â”‚
â”‚  â”‚                                                      â”‚ â”‚
â”‚  â”‚     Beitragende Faktoren fÃ¼r DIESEN Fall:          â”‚ â”‚
â”‚  â”‚     Alter < 25:      â”€â”€â”€â”€â”€â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  (-0.34)           â”‚ â”‚
â”‚  â”‚     Einkommen niedrig:â”€â”€â”€â”€â”€â–ˆâ–ˆâ–ˆâ–ˆ   (-0.28)          â”‚ â”‚
â”‚  â”‚     Gute Historie:   â–ˆâ–ˆâ–ˆâ–ˆâ”€â”€â”€â”€â”€   (+0.21)           â”‚ â”‚
â”‚  â”‚                                                      â”‚ â”‚
â”‚  â”‚     "Abgelehnt hauptsÃ¤chlich wegen jungem Alter    â”‚ â”‚
â”‚  â”‚      und niedrigem Einkommen trotz guter Historie" â”‚ â”‚
â”‚  â”‚                                                      â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                            â”‚
â”‚                                                            â”‚
â”‚  POPULÃ„RE XAI-TECHNIKEN:                                   â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                   â”‚
â”‚                                                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚                                                      â”‚ â”‚
â”‚  â”‚  SHAP (SHapley Additive exPlanations)               â”‚ â”‚
â”‚  â”‚  â€¢ Spieltheoretischer Ansatz                       â”‚ â”‚
â”‚  â”‚  â€¢ Ordnet Beitragswert jedem Feature zu            â”‚ â”‚
â”‚  â”‚  â€¢ Konsistent: gleiche Eingabe = gleiche ErklÃ¤rung â”‚ â”‚
â”‚  â”‚  â€¢ Funktioniert mit jedem Modell (model-agnostic)  â”‚ â”‚
â”‚  â”‚                                                      â”‚ â”‚
â”‚  â”‚  LIME (Local Interpretable Model-agnostic Exp)      â”‚ â”‚
â”‚  â”‚  â€¢ Erstellt lokale lineare Approximation           â”‚ â”‚
â”‚  â”‚  â€¢ StÃ¶rt Eingabe, beobachtet Ã„nderungen            â”‚ â”‚
â”‚  â”‚  â€¢ Passt einfaches Modell um Vorhersage            â”‚ â”‚
â”‚  â”‚  â€¢ Gut fÃ¼r Bild/Text-ErklÃ¤rungen                   â”‚ â”‚
â”‚  â”‚                                                      â”‚ â”‚
â”‚  â”‚  ATTENTION-VISUALISIERUNG (fÃ¼r Transformer)         â”‚ â”‚
â”‚  â”‚  Eingabe: "Der Film war absolut schrecklich"       â”‚ â”‚
â”‚  â”‚  Attention: Der Film war absolut schrecklich       â”‚ â”‚
â”‚  â”‚             â–‘   â–‘   â–‘    â–“â–“      â–ˆâ–ˆâ–ˆâ–ˆ              â”‚ â”‚
â”‚  â”‚  Modell fokussierte auf "schrecklich" und "absolut"â”‚ â”‚
â”‚  â”‚                                                      â”‚ â”‚
â”‚  â”‚  KONTRAFAKTISCHE ERKLÃ„RUNGEN                        â”‚ â”‚
â”‚  â”‚  "Was mÃ¼sste sich Ã¤ndern fÃ¼r anderes Ergebnis?"    â”‚ â”‚
â”‚  â”‚  Aktuell: Kredit ABGELEHNT                         â”‚ â”‚
â”‚  â”‚  Kontrafaktisch: "Wenn Einkommen â‚¬5000 hÃ¶her wÃ¤re, â”‚ â”‚
â”‚  â”‚  wÃ¼rde Kredit GENEHMIGT werden"                    â”‚ â”‚
â”‚  â”‚                                                      â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                            â”‚
â”‚                                                            â”‚
â”‚  REGULATORISCHE ANFORDERUNGEN:                             â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                             â”‚
â”‚                                                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚                                                      â”‚ â”‚
â”‚  â”‚  EU AI ACT (2024)                                   â”‚ â”‚
â”‚  â”‚  â”œâ”€ Hochrisiko-KI muss verstÃ¤ndlich sein           â”‚ â”‚
â”‚  â”‚  â”œâ”€ Nutzer mÃ¼ssen Ausgaben interpretieren kÃ¶nnen   â”‚ â”‚
â”‚  â”‚  â””â”€ Dokumentation des Modellverhaltens erforderlichâ”‚ â”‚
â”‚  â”‚                                                      â”‚ â”‚
â”‚  â”‚  DSGVO Artikel 22                                   â”‚ â”‚
â”‚  â”‚  â”œâ”€ Recht auf ErklÃ¤rung automatisierter Entscheid. â”‚ â”‚
â”‚  â”‚  â””â”€ "AussagekrÃ¤ftige Info Ã¼ber involvierte Logik" â”‚ â”‚
â”‚  â”‚                                                      â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## HÃ¤ufige Fragen

**F: Was ist der Unterschied zwischen ErklÃ¤rbarkeit und Interpretierbarkeit?**

A: Oft austauschbar verwendet. Technisch: Interpretierbarkeit = wie verstÃ¤ndlich ein Modell inhÃ¤rent ist; ErklÃ¤rbarkeit = Methoden um jedes Modellverhalten zu erklÃ¤ren.

**F: Verlangsamen ErklÃ¤rungen die Inferenz?**

A: Post-hoc-ErklÃ¤rungen (SHAP, LIME) fÃ¼gen Berechnung hinzu. Sie kÃ¶nnen ErklÃ¤rungen offline fÃ¼r Analyse berechnen oder NÃ¤herungsmethoden fÃ¼r Echtzeit verwenden.

**F: Sind Attention-Gewichte zuverlÃ¤ssige ErklÃ¤rungen?**

A: Umstritten. Attention zeigt wohin das Modell "geschaut" hat, beweist aber keine KausalitÃ¤t.

## Verwandte Begriffe

- AI Act â€” EU-Regulierung die ErklÃ¤rbarkeit erfordert
- Black-Box-Modell â€” Modelle die XAI-Techniken benÃ¶tigen

---

## Referenzen

> Lundberg & Lee (2017), "[A Unified Approach to Interpreting Model Predictions](https://arxiv.org/abs/1705.07874)", NeurIPS. [SHAP-Methodologie]

> Ribeiro et al. (2016), "[Why Should I Trust You? Explaining Predictions](https://arxiv.org/abs/1602.04938)", KDD. [LIME-Methodologie]

> Rudin (2019), "[Stop Explaining Black Box ML Models for High Stakes Decisions](https://arxiv.org/abs/1811.10154)", Nature Machine Intelligence.

> EuropÃ¤ische Kommission (2024), "[AI Act](https://eur-lex.europa.eu/eli/reg/2024/1689/oj)", Amtsblatt.

## References

> Lundberg & Lee (2017), "[A Unified Approach to Interpreting Model Predictions](https://arxiv.org/abs/1705.07874)", NeurIPS. [SHAP methodology]

> Ribeiro et al. (2016), "[Why Should I Trust You? Explaining the Predictions of Any Classifier](https://arxiv.org/abs/1602.04938)", KDD. [LIME methodology]

> Rudin (2019), "[Stop Explaining Black Box Machine Learning Models for High Stakes Decisions](https://arxiv.org/abs/1811.10154)", Nature Machine Intelligence. [Case for inherent interpretability]

> European Commission (2024), "[AI Act](https://eur-lex.europa.eu/eli/reg/2024/1689/oj)", Official Journal. [Regulatory requirements for explainability]
