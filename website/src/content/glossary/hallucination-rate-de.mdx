---
term: "Halluzinationsrate"
termSlug: "hallucination-rate"
short: "Anteil der Ausgaben eines Modells, die erfunden oder nicht belegt sind."
category: "ai-ml"
category_name: "KI & Machine Learning"
related: ["hallucination", "factual-consistency", "answer-grounding"]
synonyms: ["HalluzationshÃ¤ufigkeit"]
locale: "de"
draft: false
---

## Definition

Die Halluzinationsrate gibt an, wie oft ein [LLM](/de/glossary/llm/) Aussagen produziert, die nicht auf Quellen oder Fakten beruhen.

## References

> Anisha Gunjal et al. (2024), "[Detecting and Preventing Hallucinations in Large Vision Language Models](https://doi.org/10.1609/aaai.v38i16.29771)", Proceedings of the AAAI Conference on Artificial Intelligence.

> Wenyi Xiao et al. (2025), "[Detecting and Mitigating Hallucination in Large Vision Language Models via Fine-Grained AI Feedback](https://doi.org/10.1609/aaai.v39i24.34744)", Proceedings of the AAAI Conference on Artificial Intelligence.

> Ningke Li et al. (2024), "[Drowzee: Metamorphic Testing for Fact-Conflicting Hallucination Detection in Large Language Models](https://doi.org/10.1145/3689776)", Proceedings of the ACM on Programming Languages.
