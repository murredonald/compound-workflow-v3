---
term: "Hallucination rate"
termSlug: "hallucination-rate"
short: "The proportion of a modelâ€™s outputs that contain unsupported, fabricated, or false statements."
category: "ai-ml"
category_name: "AI & Machine Learning"
related: ["hallucination", "factual-consistency", "answer-grounding"]
synonyms: ["Rate of hallucinations", "Fabrication rate"]
locale: "en"
draft: false
---

## Definition

[Hallucination](/en/glossary/hallucination/) rate measures how often an LLM produces content that is not grounded in sources or reality, typically estimated via human or automated evaluation.

## References

> Anisha Gunjal et al. (2024), "[Detecting and Preventing Hallucinations in Large Vision Language Models](https://doi.org/10.1609/aaai.v38i16.29771)", Proceedings of the AAAI Conference on Artificial Intelligence.

> Wenyi Xiao et al. (2025), "[Detecting and Mitigating Hallucination in Large Vision Language Models via Fine-Grained AI Feedback](https://doi.org/10.1609/aaai.v39i24.34744)", Proceedings of the AAAI Conference on Artificial Intelligence.

> Ningke Li et al. (2024), "[Drowzee: Metamorphic Testing for Fact-Conflicting Hallucination Detection in Large Language Models](https://doi.org/10.1145/3689776)", Proceedings of the ACM on Programming Languages.
