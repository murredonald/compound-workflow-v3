---
term: "Hallucinatiegraad"
termSlug: "hallucination-rate"
short: "Het aandeel modeluitvoer dat verzonnen of niet onderbouwd is."
category: "ai-ml"
category_name: "AI & Machine Learning"
related: ["hallucination", "factual-consistency", "answer-grounding"]
synonyms: ["Hallucinatiefrequentie"]
locale: "nl"
draft: false
---

## Definitie

De hallucinatiegraad drukt uit hoe vaak een [LLM](/nl/glossary/llm/) uitspraken doet die niet gesteund worden door bronnen of feiten.

## References

> Anisha Gunjal et al. (2024), "[Detecting and Preventing Hallucinations in Large Vision Language Models](https://doi.org/10.1609/aaai.v38i16.29771)", Proceedings of the AAAI Conference on Artificial Intelligence.

> Wenyi Xiao et al. (2025), "[Detecting and Mitigating Hallucination in Large Vision Language Models via Fine-Grained AI Feedback](https://doi.org/10.1609/aaai.v39i24.34744)", Proceedings of the AAAI Conference on Artificial Intelligence.

> Ningke Li et al. (2024), "[Drowzee: Metamorphic Testing for Fact-Conflicting Hallucination Detection in Large Language Models](https://doi.org/10.1145/3689776)", Proceedings of the ACM on Programming Languages.
