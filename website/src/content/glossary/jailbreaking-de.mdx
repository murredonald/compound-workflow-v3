---
term: "Jailbreaking"
termSlug: "jailbreaking"
short: "Das gezielte Gestalten von Prompts oder Eingaben, um die Sicherheits- und Richtliniengrenzen eines KI-Systems zu umgehen."
category: "ai-ml"
category_name: "KI & Machine Learning"
related: []
synonyms: []
locale: "de"
draft: false
---

## Definition

Jailbreaking bezeichnet Techniken, bei denen [Prompts](/de/glossary/prompt/) oder Eingaben absichtlich so formuliert werden, dass ein Sprachmodell seine Sicherheitsrichtlinien umgeht oder ignoriert und dadurch eigentlich eingeschrÃ¤nkte oder problematische Inhalte erzeugt.

## References

> Patrick Chao et al. (2023), "[Jailbreaking Black Box Large Language Models in Twenty Queries](https://arxiv.org/abs/2310.08419)", 2025 IEEE Conference on Secure and Trustworthy Machine Learning (SaTML).

> Yichen Gong et al. (2023), "[FigStep: Jailbreaking Large Vision-language Models via Typographic Visual Prompts](https://arxiv.org/abs/2311.05608)", AAAI Conference on Artificial Intelligence.

> Rishabh Bhardwaj et al. (2023), "[Red-Teaming Large Language Models using Chain of Utterances for Safety-Alignment](https://arxiv.org/abs/2308.09662)", arXiv.
