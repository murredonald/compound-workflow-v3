---
term: "Jailbreaking"
termSlug: "jailbreaking"
short: "The practice of crafting prompts or inputs to bypass an AI system's safety and policy constraints."
category: "ai-ml"
category_name: "AI & Machine Learning"
related: ["prompt-injection", "guardrails", "responsible-ai"]
synonyms: ["Model jailbreaking", "Safety bypassing"]
locale: "en"
draft: false
---

## Definition

Jailbreaking refers to techniques that deliberately manipulate [prompts](/en/glossary/prompt/) or inputs so that a language model ignores or evades its safety policies and produces restricted or harmful content.

## References

> Patrick Chao et al. (2023), "[Jailbreaking Black Box Large Language Models in Twenty Queries](https://arxiv.org/abs/2310.08419)", 2025 IEEE Conference on Secure and Trustworthy Machine Learning (SaTML).

> Yichen Gong et al. (2023), "[FigStep: Jailbreaking Large Vision-language Models via Typographic Visual Prompts](https://arxiv.org/abs/2311.05608)", AAAI Conference on Artificial Intelligence.

> Rishabh Bhardwaj et al. (2023), "[Red-Teaming Large Language Models using Chain of Utterances for Safety-Alignment](https://arxiv.org/abs/2308.09662)", arXiv.
