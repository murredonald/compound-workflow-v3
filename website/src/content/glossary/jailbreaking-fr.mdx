---
term: "Jailbreaking"
termSlug: "jailbreaking"
short: "La pratique consistant à concevoir des prompts ou entrées pour contourner les garde-fous et politiques d'un système d'IA."
category: "ai-ml"
category_name: "IA & Machine Learning"
related: []
synonyms: []
locale: "fr"
draft: false
---

## Définition

Le jailbreaking désigne les techniques qui manipulent volontairement les [prompts](/fr/glossary/prompt/) ou les entrées afin qu'un modèle de langage ignore ou contourne ses politiques de sécurité et génère du contenu restreint ou inapproprié.

## References

> Patrick Chao et al. (2023), "[Jailbreaking Black Box Large Language Models in Twenty Queries](https://arxiv.org/abs/2310.08419)", 2025 IEEE Conference on Secure and Trustworthy Machine Learning (SaTML).

> Yichen Gong et al. (2023), "[FigStep: Jailbreaking Large Vision-language Models via Typographic Visual Prompts](https://arxiv.org/abs/2311.05608)", AAAI Conference on Artificial Intelligence.

> Rishabh Bhardwaj et al. (2023), "[Red-Teaming Large Language Models using Chain of Utterances for Safety-Alignment](https://arxiv.org/abs/2308.09662)", arXiv.
