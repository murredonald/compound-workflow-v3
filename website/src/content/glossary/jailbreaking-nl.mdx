---
term: "Jailbreaking"
termSlug: "jailbreaking"
short: "Het bewust ontwerpen van prompts of inputs om de veiligheids- en beleidskaders van een AI-systeem te omzeilen."
category: "ai-ml"
category_name: "AI & Machine Learning"
related: []
synonyms: []
locale: "nl"
draft: false
---

## Definitie

Jailbreaking verwijst naar technieken waarmee [prompts](/nl/glossary/prompt/) of invoer zo worden gemanipuleerd dat een taalmodel zijn veiligheidsbeleid negeert of ontwijkt en toch verboden of gevoelige inhoud produceert.

## References

> Patrick Chao et al. (2023), "[Jailbreaking Black Box Large Language Models in Twenty Queries](https://arxiv.org/abs/2310.08419)", 2025 IEEE Conference on Secure and Trustworthy Machine Learning (SaTML).

> Yichen Gong et al. (2023), "[FigStep: Jailbreaking Large Vision-language Models via Typographic Visual Prompts](https://arxiv.org/abs/2311.05608)", AAAI Conference on Artificial Intelligence.

> Rishabh Bhardwaj et al. (2023), "[Red-Teaming Large Language Models using Chain of Utterances for Safety-Alignment](https://arxiv.org/abs/2308.09662)", arXiv.
