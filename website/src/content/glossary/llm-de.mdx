---
term: "LLM"
termSlug: "llm"
short: "Large Language Models sind KI-Systeme, die auf riesigen Textdaten trainiert wurden, um menschenähnlichen Text zu verstehen und zu generieren."
category: "ai-ml"
category_name: "KI & ML"
related: ["rag"]
synonyms: ["Large Language Model", "Großes Sprachmodell"]
locale: "de"
draft: false
---

## Was ist ein LLM?

Ein **Large Language Model (LLM)** ist ein System der künstlichen Intelligenz, das auf riesigen Mengen von Textdaten trainiert wurde, um menschliche Sprache zu verstehen, zu generieren und darüber zu argumentieren. Diese Modelle lernen Muster aus Milliarden von Wörtern und ermöglichen ihnen, Aufgaben wie das Beantworten von Fragen, das Zusammenfassen von Dokumenten und das Generieren kohärenter Texte auszuführen.

## Hauptmerkmale

- **Skalierung** — trainiert auf Milliarden oder Billionen von Tokens (Wörter/Teilwörter)
- **Emergente Fähigkeiten** — zeigen Fähigkeiten, die nicht explizit programmiert wurden
- **Kontextverständnis** — halten Kohärenz über lange Gespräche hinweg
- **Transfer Learning** — wenden Wissen über verschiedene Bereiche an

## Einschränkungen für den professionellen Einsatz

Obwohl leistungsfähig, haben Standard-LLMs erhebliche Einschränkungen für Steuerfachleute:

- **Wissensstichtag** — Trainingsdaten haben ein festes Datum, neuere Änderungen fehlen
- **[Halluzination](/de/glossary/hallucination/)** — können plausible, aber falsche Informationen generieren
- **Keine [Quellenangabe](/de/glossary/citation/)** — können nicht zitieren, woher Informationen stammen
- **Jurisdiktionsblindheit** — fehlendes tiefes Verständnis spezifischer Rechtssysteme

## Wie diese Einschränkungen in der Praxis adressiert werden

Spezialisierte KI-Systeme kombinieren LLM-Fähigkeiten mit [RAG](/de/glossary/rag/)-Architektur, Vertrauensbewertung und domänenspezifischem Training, um zuverlässige, quellenbasierte Antworten zu liefern. Anstatt sich ausschließlich auf parametrisches Wissen zu verlassen, rufen diese Systeme aktuelle Gesetzgebung ab und verifizieren Ergebnisse gegen autoritative Quellen.

## References

- Zhao et al. (2023), "[A Survey of Large Language Models](https://arxiv.org/abs/2303.18223)", arXiv.

- Brown et al. (2020), "[Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165)", NeurIPS.

- Touvron et al. (2023), "[LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/abs/2302.13971)", arXiv.
