---
term: "LLM"
termSlug: "llm"
short: "Large Language Models are AI systems trained on vast text data to understand and generate human-like text, powering modern conversational AI."
category: "ai-ml"
category_name: "AI & ML"
related: ["rag"]
synonyms: ["Large Language Model", "Large Language Models"]
locale: "en"
draft: false
---

## What is an LLM?

A **Large Language Model (LLM)** is an artificial intelligence system trained on massive amounts of text data to understand, generate, and reason about human language. These models learn patterns from billions of words, enabling them to perform tasks like answering questions, summarizing documents, and generating coherent text.

## Key characteristics

- **Scale** — trained on billions or trillions of tokens (words/subwords)
- **Emergent capabilities** — exhibit abilities not explicitly programmed
- **Context understanding** — maintain coherence across long conversations
- **Transfer learning** — apply knowledge across different domains

## Limitations for professional use

While powerful, standard LLMs have significant limitations for tax professionals:

- **Knowledge cutoff** — training data has a fixed date, missing recent changes
- **[Hallucination](/en/glossary/hallucination/)** — may generate plausible but incorrect information
- **No [source attribution](/en/glossary/attribution/)** — cannot cite where information originated
- **Jurisdiction blindness** — lack deep understanding of specific legal systems

## How these limitations are addressed in practice

Specialized AI systems combine LLM capabilities with RAG architecture, [confidence scoring](/en/glossary/confidence-scoring/), and domain-specific training to provide reliable, source-backed answers. Rather than relying on parametric knowledge alone, these systems retrieve current legislation and verify outputs against authoritative sources.

## References

- Zhao et al. (2023), "[A Survey of Large Language Models](https://arxiv.org/abs/2303.18223)", arXiv.

- Brown et al. (2020), "[Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165)", NeurIPS.

- Touvron et al. (2023), "[LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/abs/2302.13971)", arXiv.
