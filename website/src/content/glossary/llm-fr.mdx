---
term: "LLM"
termSlug: "llm"
short: "Les grands modèles de langage sont des systèmes d'IA entraînés sur de vastes données textuelles pour comprendre et générer du texte semblable à celui des humains."
category: "ai-ml"
category_name: "IA & ML"
related: ["rag"]
synonyms: ["Large Language Model", "Grand modèle de langage"]
locale: "fr"
draft: false
---

## Qu'est-ce qu'un LLM ?

Un **Large Language Model (LLM)** est un système d'intelligence artificielle entraîné sur d'énormes quantités de données textuelles pour comprendre, générer et raisonner sur le langage humain. Ces modèles apprennent des motifs à partir de milliards de mots, leur permettant d'effectuer des tâches comme répondre à des questions, résumer des documents et générer du texte cohérent.

## Caractéristiques principales

- **Échelle** — entraînés sur des milliards ou des trillions de tokens (mots/sous-mots)
- **Capacités émergentes** — présentent des aptitudes non explicitement programmées
- **Compréhension du contexte** — maintiennent la cohérence sur de longues conversations
- **Apprentissage par transfert** — appliquent les connaissances à différents domaines

## Limitations pour l'usage professionnel

Bien que puissants, les LLM standard ont des limitations significatives pour les professionnels de la fiscalité :

- **Date limite de connaissance** — les données d'entraînement ont une date fixe, manquant les changements récents
- **[Hallucination](/fr/glossary/hallucination/)** — peuvent générer des informations plausibles mais incorrectes
- **Pas d'[attribution de source](/fr/glossary/attribution/)** — ne peuvent pas citer d'où provient l'information
- **Aveuglement juridictionnel** — manquent de compréhension approfondie des systèmes juridiques spécifiques

## Comment ces limitations sont abordées en pratique

Les systèmes d'IA spécialisés combinent les capacités LLM avec l'architecture [RAG](/fr/glossary/rag/), le scoring de confiance et une formation spécifique au domaine pour fournir des réponses fiables et sourcées. Au lieu de s'appuyer uniquement sur les connaissances paramétriques, ces systèmes récupèrent la législation actuelle et vérifient les résultats contre des sources faisant autorité.

## References

- Zhao et al. (2023), "[A Survey of Large Language Models](https://arxiv.org/abs/2303.18223)", arXiv.

- Brown et al. (2020), "[Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165)", NeurIPS.

- Touvron et al. (2023), "[LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/abs/2302.13971)", arXiv.
