---
term: "LLM"
termSlug: "llm"
short: "Large Language Models zijn AI-systemen getraind op enorme hoeveelheden tekstdata om mensachtige tekst te begrijpen en genereren."
category: "ai-ml"
category_name: "AI & ML"
related: ["rag"]
synonyms: ["Large Language Model", "Groot Taalmodel"]
locale: "nl"
draft: false
---

## Wat is een LLM?

Een **Large Language Model (LLM)** is een kunstmatige intelligentiesysteem dat is getraind op enorme hoeveelheden tekstdata om menselijke taal te begrijpen, te genereren en erover te redeneren. Deze modellen leren patronen uit miljarden woorden, waardoor ze taken kunnen uitvoeren zoals het beantwoorden van vragen, het samenvatten van documenten en het genereren van coherente tekst.

## Belangrijkste kenmerken

- **Schaal** — getraind op miljarden of biljoenen tokens (woorden/subwoorden)
- **Emergente mogelijkheden** — vertonen capaciteiten die niet expliciet geprogrammeerd zijn
- **Contextbegrip** — behouden coherentie over lange gesprekken
- **Transfer learning** — passen kennis toe over verschillende domeinen

## Beperkingen voor professioneel gebruik

Hoewel krachtig, hebben standaard LLM's significante beperkingen voor fiscalisten:

- **Kennisgrens** — trainingsdata heeft een vaste datum, mist recente wijzigingen
- **[Hallucinatie](/nl/glossary/hallucination/)** — kan plausibele maar onjuiste informatie genereren
- **Geen [bronvermelding](/nl/glossary/citation/)** — kan niet citeren waar informatie vandaan komt
- **Jurisdictieblindheid** — missen diep begrip van specifieke rechtssystemen

## Hoe deze beperkingen in de praktijk worden aangepakt

Gespecialiseerde AI-systemen combineren LLM-mogelijkheden met [RAG](/nl/glossary/rag/)-architectuur, betrouwbaarheidsscores en domeinspecifieke training om betrouwbare, brongebaseerde antwoorden te bieden. In plaats van uitsluitend te vertrouwen op parametrische kennis, halen deze systemen actuele wetgeving op en verifiëren outputs tegen gezaghebbende bronnen.

## References

- Zhao et al. (2023), "[A Survey of Large Language Models](https://arxiv.org/abs/2303.18223)", arXiv.

- Brown et al. (2020), "[Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165)", NeurIPS.

- Touvron et al. (2023), "[LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/abs/2302.13971)", arXiv.
