---
term: "Modellrobustheit"
termSlug: "model-robustness"
short: "Wie gut ein Modell seine Leistung bei Rauschen, Verschiebungen oder adversarialen Eingaben beibehÃ¤lt."
category: "ai-ml"
category_name: "KI & Machine Learning"
related: ["adversarial-testing", "model-drift", "uncertainty-estimation"]
synonyms: ["Robustheit", "Modellresilienz"]
locale: "de"
draft: false
---

## References

> Yinpeng Dong et al. (2018), "[Boosting Adversarial Attacks with Momentum](https://doi.org/10.1109/cvpr.2018.00957)", .

> Jiawei Su et al. (2019), "[One Pixel Attack for Fooling Deep Neural Networks](https://doi.org/10.1109/tevc.2019.2890858)", IEEE Transactions on Evolutionary Computation.

> Kimin Lee et al. (2018), "[A Simple Unified Framework for Detecting Out-of-Distribution Samples and Adversarial Attacks](https://doi.org/10.48550/arxiv.1807.03888)", arXiv.
