---
term: "Perplexité"
termSlug: "perplexity"
short: "Une métrique mesurant à quel point un modèle de langage prédit bien le texte, avec des valeurs plus basses indiquant une meilleure capacité de prédiction."
category: "ai-ml"
category_name: "IA & Machine Learning"
related: ["loss-function", "llm", "inference", "tokenization"]
synonyms: ["PPL", "Perplexité du modèle", "Perplexité du modèle de langage"]
locale: "fr"
draft: false
---

## Définition

La perplexité est une mesure de la qualité avec laquelle un modèle de probabilité prédit un échantillon. Pour les modèles de langage, elle représente l'incertitude du modèle lors de la [prédiction](/fr/glossary/inference/) du prochain token—une perplexité plus basse signifie que le modèle est moins "perplexe" ou plus confiant. Mathématiquement, la perplexité est la moyenne exponentielle de la log-vraisemblance négative par token.

## Pourquoi c'est important

La perplexité est une métrique d'évaluation fondamentale :

- **Comparaison de modèles** — comparer différents modèles sur le même jeu de données
- **Suivi d'entraînement** — suivre l'amélioration pendant l'entraînement
- **Évaluation de domaine** — mesurer à quel point le modèle correspond à un texte spécifique
- **Impact de [quantification](/fr/glossary/quantization/)** — évaluer la perte de qualité due à la compression
- **Échelle interprétable** — peut être comprise comme taille de vocabulaire effective

La perplexité aide à répondre : "À quel point le modèle est-il surpris par ce texte ?"

## Comment ça fonctionne

```
┌────────────────────────────────────────────────────────────┐
│                        PERPLEXITÉ                          │
├────────────────────────────────────────────────────────────┤
│                                                            │
│  Formule: PPL = exp(-1/N × Σ log P(token_i | contexte))    │
│                                                            │
│  Exemple: "Le chat dort"                                   │
│  ───────────────────────                                   │
│                                                            │
│  Token        P(token|contexte)   log P                    │
│  ─────────────────────────────────────────                 │
│  "Le"         0.10                -2.30                    │
│  "chat"       0.25                -1.39                    │
│  "dort"       0.40                -0.92                    │
│                                                            │
│  Moyenne log P = (-2.30 + -1.39 + -0.92) / 3 = -1.54       │
│  Perplexité = exp(1.54) = 4.66                             │
│                                                            │
│  ┌────────────────────────────────────────────────┐        │
│  │  INTERPRÉTATION:                               │        │
│  │                                                │        │
│  │  PPL ≈ "choix effectifs par position"        │        │
│  │                                                │        │
│  │  PPL = 1:   Modèle 100% certain              │        │
│  │  PPL = 10:  ~10 options également probables  │        │
│  │  PPL = 50k: Aléatoire (taille vocab)=pas app.│        │
│  └────────────────────────────────────────────────┘        │
│                                                            │
│  ÉCHELLE DE PERPLEXITÉ:                                    │
│  ──────────────────────                                    │
│  │◄────────────────────────────────────────────►│          │
│  1        10        50       100      1000    50000        │
│  Parfait   Super    Bon     Correct   Faible  Aléatoire    │
│                                                            │
│  VALEURS TYPIQUES:                                         │
│  ─────────────────                                         │
│  GPT-4 sur texte courant:  ~10-20                          │
│  Petit modèle:             ~50-100                         │
│  Écart de domaine:         ~100-500                        │
│  Modèle non entraîné:      ~taille vocab                   │
│                                                            │
└────────────────────────────────────────────────────────────┘
```

**[Références](/fr/glossary/citation/) de perplexité:**
| Plage | Qualité | Interprétation |
|-------|---------|----------------|
| 1-10 | Excellent | Texte très prévisible |
| 10-30 | Très bon | Typique pour LLMs forts |
| 30-50 | Bon | Modèle raisonnable |
| 50-100 | Modéré | Peut nécessiter amélioration |
| 100+ | Faible | Problèmes significatifs ou écart de domaine |

## Questions fréquentes

**Q : Qu'est-ce qu'un "bon" score de perplexité ?**

R : Cela dépend du jeu de données et de la taille du modèle. Les modèles état de l'art atteignent une perplexité ~15-25 sur les benchmarks standards comme WikiText. Dans un projet, concentrez-vous sur les améliorations relatives plutôt que les chiffres absolus.

**Q : La perplexité peut-elle comparer des modèles avec différents tokenizers ?**

R : Pas directement—différents tokenizers produisent différents nombres de tokens pour le même texte. Comparez les modèles utilisant le même tokenizer, ou normalisez par nombre de caractères/mots au lieu du nombre de tokens.

**Q : Pourquoi la perplexité peut être basse mais la qualité de génération mauvaise ?**

R : La perplexité mesure la précision moyenne de prédiction, pas la qualité de sortie. Un modèle peut avoir une perplexité basse en prédisant bien les mots communs tout en échouant à la génération cohérente longue. Utilisez la perplexité avec d'autres métriques.

**Q : Comment la perplexité se rapporte-t-elle à la perte d'entropie croisée ?**

R : Perplexité = exp(entropie croisée). Elles mesurent la même chose sur différentes échelles. L'entropie croisée est typiquement utilisée pendant l'entraînement (calcul de gradient plus facile); la perplexité est plus interprétable pour les rapports.

## Termes associés

- [Fonction de Perte](/fr/glossary/loss-function/) — objectif d'entraînement
- [LLM](/fr/glossary/llm/) — modèles de langage évalués
- [Tokenisation](/fr/glossary/tokenization/) — affecte le calcul de perplexité
- [Fine-tuning](/fr/glossary/fine-tuning/) — améliore la perplexité du domaine

---

## Références

> Jelinek et al. (1977), "[Perplexity—a measure of the difficulty of speech recognition tasks](https://ieeexplore.ieee.org/document/1170781)", JASA. [Article fondateur]

> Merity et al. (2017), "[Regularizing and Optimizing LSTM Language Models](https://arxiv.org/abs/1708.02182)", ICLR. [1 500+ citations]

> Brown et al. (2020), "[Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165)", NeurIPS. [15 000+ citations]

> Radford et al. (2019), "[Language Models are Unsupervised Multitask Learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)", OpenAI. [10 000+ citations]

## References

> Jelinek et al. (1977), "[Perplexity—a measure of the difficulty of speech recognition tasks](https://ieeexplore.ieee.org/document/1170781)", JASA. [Foundational paper]

> Merity et al. (2017), "[Regularizing and Optimizing LSTM Language Models](https://arxiv.org/abs/1708.02182)", ICLR. [1,500+ citations]

> Brown et al. (2020), "[Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165)", NeurIPS. [15,000+ citations]

> Radford et al. (2019), "[Language Models are Unsupervised Multitask Learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)", OpenAI. [10,000+ citations]
