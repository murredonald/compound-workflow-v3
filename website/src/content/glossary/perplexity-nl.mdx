---
term: "Perplexiteit"
termSlug: "perplexity"
short: "Een metriek die meet hoe goed een taalmodel tekst voorspelt, waarbij lagere waarden betere voorspellingsabiliteit aangeven."
category: "ai-ml"
category_name: "AI & Machine Learning"
related: ["loss-function", "llm", "inference", "tokenization"]
synonyms: ["PPL", "Model perplexiteit", "Taalmodel perplexiteit"]
locale: "nl"
draft: false
---

## Definitie

Perplexiteit is een meting van hoe goed een kansmodel een sample voorspelt. Voor taalmodellen vertegenwoordigt het de onzekerheid van het model bij het voorspellen van het volgende token—lagere perplexiteit betekent dat het model minder "perplex" of meer zeker is. Wiskundig is perplexiteit de geëxponentieerde gemiddelde negatieve log-likelihood per token.

## Waarom het belangrijk is

Perplexiteit is een fundamentele evaluatiemetriek:

- **Modelvergelijking** — vergelijk verschillende modellen op dezelfde dataset
- **Trainingsmonitoring** — volg verbetering tijdens training
- **Domeinbeoordeling** — meet hoe goed model past bij specifieke tekst
- **Quantisatie-impact** — evalueer kwaliteitsverlies door compressie
- **Interpreteerbare schaal** — kan begrepen worden als effectieve vocabulairegrootte

Perplexiteit helpt beantwoorden: "Hoe verrast is het model door deze tekst?"

## Hoe het werkt

```
┌────────────────────────────────────────────────────────────┐
│                       PERPLEXITEIT                         │
├────────────────────────────────────────────────────────────┤
│                                                            │
│  Formule: PPL = exp(-1/N × Σ log P(token_i | context))     │
│                                                            │
│  Voorbeeld: "De kat zat"                                   │
│  ───────────────────────                                   │
│                                                            │
│  Token        P(token|context)    log P                    │
│  ─────────────────────────────────────────                 │
│  "De"         0.10                -2.30                    │
│  "kat"        0.25                -1.39                    │
│  "zat"        0.40                -0.92                    │
│                                                            │
│  Gemiddelde log P = (-2.30 + -1.39 + -0.92) / 3 = -1.54    │
│  Perplexiteit = exp(1.54) = 4.66                           │
│                                                            │
│  ┌────────────────────────────────────────────────┐        │
│  │  INTERPRETATIE:                                │        │
│  │                                                │        │
│  │  PPL ≈ "effectieve keuzes per positie"       │        │
│  │                                                │        │
│  │  PPL = 1:   Model is 100% zeker              │        │
│  │  PPL = 10:  ~10 gelijk waarschijnlijke opties│        │
│  │  PPL = 50k: Willekeurig (vocab) = geen leren │        │
│  └────────────────────────────────────────────────┘        │
│                                                            │
│  PERPLEXITEITSSCHAAL:                                      │
│  ────────────────────                                      │
│  │◄────────────────────────────────────────────►│          │
│  1        10        50       100      1000    50000        │
│  Perfect   Goed     Redelijk Oké      Slecht  Willekeur    │
│                                                            │
│  TYPISCHE WAARDEN:                                         │
│  ─────────────────                                         │
│  GPT-4 op gewone tekst:    ~10-20                          │
│  Klein model:              ~50-100                         │
│  Domein mismatch:          ~100-500                        │
│  Ongetraind model:         ~vocab grootte                  │
│                                                            │
└────────────────────────────────────────────────────────────┘
```

**Perplexiteit benchmarks:**
| Bereik | Kwaliteit | Interpretatie |
|--------|-----------|---------------|
| 1-10 | Uitstekend | Zeer voorspelbare tekst |
| 10-30 | Zeer goed | Typisch voor sterke LLMs |
| 30-50 | Goed | Redelijk model |
| 50-100 | Matig | Kan verbetering gebruiken |
| 100+ | Slecht | Significante problemen of domein mismatch |

## Veelgestelde vragen

**V: Wat is een "goede" perplexiteitsscore?**

A: Het hangt af van de dataset en modelgrootte. State-of-the-art modellen behalen perplexiteit ~15-25 op standaard benchmarks zoals WikiText. Binnen een project, focus op relatieve verbeteringen in plaats van absolute getallen.

**V: Kan perplexiteit modellen met verschillende tokenizers vergelijken?**

A: Niet direct—verschillende tokenizers produceren verschillende aantallen tokens voor dezelfde tekst. Vergelijk modellen met dezelfde tokenizer, of normaliseer op karakter/woordaantal in plaats van tokenaantal.

**V: Waarom kan perplexiteit laag zijn maar generatiekwaliteit slecht?**

A: Perplexiteit meet gemiddelde voorspellingsnauwkeurigheid, niet outputkwaliteit. Een model kan lage perplexiteit hebben door gewone woorden goed te voorspellen terwijl het faalt bij coherente langvormige generatie. Gebruik perplexiteit naast andere metrieken.

**V: Hoe relateert perplexiteit tot cross-entropie loss?**

A: Perplexiteit = exp(cross-entropie). Ze meten hetzelfde op verschillende schalen. Cross-entropie wordt typisch tijdens training gebruikt (makkelijkere gradiëntberekening); perplexiteit is meer interpreteerbaar voor rapportage.

## Gerelateerde termen

- [Verliesfunctie](/nl/glossary/loss-function/) — trainingsdoel
- [LLM](/nl/glossary/llm/) — taalmodellen die geëvalueerd worden
- [Tokenisatie](/nl/glossary/tokenization/) — beïnvloedt perplexiteitsberekening
- [Fine-tuning](/nl/glossary/fine-tuning/) — verbetert domeinperplexiteit

---

## Referenties

> Jelinek et al. (1977), "[Perplexity—a measure of the difficulty of speech recognition tasks](https://ieeexplore.ieee.org/document/1170781)", JASA. [Fundamenteel paper]

> Merity et al. (2017), "[Regularizing and Optimizing LSTM Language Models](https://arxiv.org/abs/1708.02182)", ICLR. [1.500+ [citaties](/nl/glossary/citation/)]

> Brown et al. (2020), "[Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165)", NeurIPS. [15.000+ citaties]

> Radford et al. (2019), "[Language Models are Unsupervised Multitask Learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)", OpenAI. [10.000+ citaties]

## References

> Jelinek et al. (1977), "[Perplexity—a measure of the difficulty of speech recognition tasks](https://ieeexplore.ieee.org/document/1170781)", JASA. [Foundational paper]

> Merity et al. (2017), "[Regularizing and Optimizing LSTM Language Models](https://arxiv.org/abs/1708.02182)", ICLR. [1,500+ citations]

> Brown et al. (2020), "[Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165)", NeurIPS. [15,000+ citations]

> Radford et al. (2019), "[Language Models are Unsupervised Multitask Learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)", OpenAI. [10,000+ citations]
