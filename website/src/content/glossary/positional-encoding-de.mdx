---
term: "Positional Encoding"
termSlug: "positional-encoding"
short: "Verfahren in Transformer-Modellen, um Positionsinformationen zu ansonsten reihenfolgenblinden Embeddings hinzuzufügen."
category: "ai-ml"
category_name: "KI & Machine Learning"
related: []
synonyms: []
locale: "de"
draft: false
---

## Definition

Positional Encoding ist ein zusätzlicher Vektor, der zu Token-[Embeddings](/de/glossary/embeddings/) addiert oder mit ihnen verknüpft wird, damit ein [Transformer](/de/glossary/transformer-architecture/) die Reihenfolge und relativen Positionen von Tokens in einer Sequenz unterscheiden kann.

## References

- Vaswani et al. (2017), "[Attention Is All You Need](https://arxiv.org/abs/1706.03762)", NeurIPS.

- Su et al. (2023), "[RoFormer: Enhanced Transformer with Rotary Position Embedding](https://doi.org/10.1016/j.neucom.2023.127063)", Neurocomputing.

- Press et al. (2022), "[Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation](https://arxiv.org/abs/2108.12409)", ICLR.
