---
term: "Positional encoding"
termSlug: "positional-encoding"
short: "A technique used in transformer models to inject information about token positions into otherwise order-agnostic embeddings."
category: "ai-ml"
category_name: "AI & Machine Learning"
related: ["transformer-architecture", "attention-mechanism", "context-window"]
synonyms: ["Position encoding", "Positional representation"]
locale: "en"
draft: false
---

## Definition

Positional encoding is an additional vector added to or concatenated with token [embeddings](/en/glossary/embeddings/) so that a [transformer model](/en/glossary/transformer-architecture/) can distinguish the order and relative positions of tokens in a sequence.

## References

- Vaswani et al. (2017), "[Attention Is All You Need](https://arxiv.org/abs/1706.03762)", NeurIPS.

- Su et al. (2023), "[RoFormer: Enhanced Transformer with Rotary Position Embedding](https://doi.org/10.1016/j.neucom.2023.127063)", Neurocomputing.

- Press et al. (2022), "[Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation](https://arxiv.org/abs/2108.12409)", ICLR.
