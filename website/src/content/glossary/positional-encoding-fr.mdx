---
term: "Encodage positionnel"
termSlug: "positional-encoding"
short: "Technique des transformeurs pour injecter des informations de position de tokens dans des embeddings autrement insensibles à l'ordre."
category: "ai-ml"
category_name: "IA & Machine Learning"
related: []
synonyms: []
locale: "fr"
draft: false
---

## Définition

L'encodage positionnel ajoute ou concatène un vecteur supplémentaire aux [embeddings](/fr/glossary/embeddings/) de tokens afin qu'un modèle de type transformeur puisse distinguer l'ordre et la position relative des tokens dans une séquence.

## References

- Vaswani et al. (2017), "[Attention Is All You Need](https://arxiv.org/abs/1706.03762)", NeurIPS.

- Su et al. (2023), "[RoFormer: Enhanced Transformer with Rotary Position Embedding](https://doi.org/10.1016/j.neucom.2023.127063)", Neurocomputing.

- Press et al. (2022), "[Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation](https://arxiv.org/abs/2108.12409)", ICLR.
