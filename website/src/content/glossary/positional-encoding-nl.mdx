---
term: "Positional encoding"
termSlug: "positional-encoding"
short: "Techniek in transformer-modellen om informatie over tokenposities toe te voegen aan verder volgorde-ongevoelige embeddings."
category: "ai-ml"
category_name: "AI & Machine Learning"
related: []
synonyms: []
locale: "nl"
draft: false
---

## Definitie

Positional encoding is een extra vector die aan token-[embeddings](/nl/glossary/embeddings/) wordt toegevoegd of ermee wordt geconcateneerd zodat een [transformer](/nl/glossary/transformer-architecture/) het verschil tussen posities en volgorde van tokens in een sequentie kan leren.

## References

- Vaswani et al. (2017), "[Attention Is All You Need](https://arxiv.org/abs/1706.03762)", NeurIPS.

- Su et al. (2023), "[RoFormer: Enhanced Transformer with Rotary Position Embedding](https://doi.org/10.1016/j.neucom.2023.127063)", Neurocomputing.

- Press et al. (2022), "[Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation](https://arxiv.org/abs/2108.12409)", ICLR.
