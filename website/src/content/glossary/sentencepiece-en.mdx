---
term: "SentencePiece"
termSlug: "sentencepiece"
short: "A language-agnostic subword tokenization library that learns a vocabulary directly from raw text."
category: "ai-ml"
category_name: "AI & Machine Learning"
related: ["tokenization", "byte-pair-encoding", "llm"]
synonyms: ["SentencePiece tokenizer"]
locale: "en"
draft: false
---

## Definition

SentencePiece is an open-source tokenizer that treats input as a raw byte or Unicode stream and learns a subword vocabulary (e.g., [BPE](/en/glossary/byte-pair-encoding/) or unigram) without relying on language-specific pre-[tokenization](/en/glossary/tokenization/) rules.

## References

- Kudo & Richardson (2018), "[SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing](https://doi.org/10.18653/v1/D18-2012)", EMNLP.

- Kudo (2018), "[Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates](https://doi.org/10.18653/v1/P18-1007)", ACL.

- Sennrich et al. (2016), "[Neural Machine Translation of Rare Words with Subword Units](https://doi.org/10.18653/v1/P16-1162)", ACL.
