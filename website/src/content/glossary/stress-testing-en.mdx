---
term: "Stress testing"
termSlug: "stress-testing"
short: "Evaluating how an AI system behaves under extreme or degraded conditions."
category: "ai-ml"
category_name: "AI & Machine Learning"
related: ["adversarial-testing", "retrieval-latency", "continuous-evaluation"]
synonyms: ["Load testing for AI", "Resilience testing"]
locale: "en"
draft: false
---

## Definition

Stress testing evaluates how an AI system behaves under extreme or degraded conditions that exceed normal operating parameters. This includes high query volumes that saturate system capacity, noisy or adversarial inputs, partial infrastructure failures, and unusual query patterns. The goal is not to test normal performance (that is what standard evaluation does) but to find the system's breaking points, understand its failure modes, and verify that it degrades gracefully rather than catastrophically when pushed beyond its limits.

## Why it matters

- **Failure mode discovery** — stress testing reveals how the system fails: does it slow down gracefully, return errors cleanly, or produce silently incorrect answers? Each failure mode has different implications for user trust and safety
- **Capacity planning** — understanding the system's throughput limits and degradation curve informs infrastructure sizing and scaling decisions
- **Adversarial resilience** — stress testing with adversarial inputs reveals vulnerabilities to prompt injection, query manipulation, and other attacks that might not surface in normal operation
- **Regulatory readiness** — the EU AI Act requires high-risk AI systems to maintain performance under "reasonably foreseeable conditions of use"; stress testing provides evidence for this requirement

## How it works

Stress testing covers several dimensions:

**Load testing** gradually increases query volume until the system's performance degrades. Metrics tracked include response time (how much does latency increase?), error rate (do requests start failing?), and answer quality (do answers become less accurate under load?). The test identifies the maximum sustainable throughput and the degradation pattern beyond it.

**Input perturbation testing** sends inputs that deviate from normal patterns: extremely long queries, queries in unexpected languages, queries with special characters or formatting, and queries designed to confuse the retrieval or generation layers. The goal is to verify that the system handles unusual inputs without crashing or producing dangerous outputs.

**Infrastructure failure testing** simulates component failures: a vector database shard going offline, an embedding model service becoming unavailable, or a network partition between system components. The test verifies that the system detects failures, routes around them when possible, and communicates degradation to users clearly.

**Adversarial testing** uses deliberately crafted inputs that attempt to exploit system weaknesses: prompt injection attacks, queries designed to extract training data, and inputs that attempt to bypass safety guardrails. This overlaps with security testing but focuses specifically on AI-specific vulnerabilities.

**Edge case testing** targets known difficult scenarios: ambiguous queries, queries at the boundary of the system's scope, queries about topics where the knowledge base has sparse coverage, and queries that require reasoning across multiple sources.

Results are documented in terms of discovered failure modes, breaking points, and remediation priorities.

## Common questions

**Q: How is stress testing different from regression testing?**

A: Regression testing verifies that the system maintains quality under normal conditions after changes. Stress testing pushes the system beyond normal conditions to find its limits. Regression testing asks "is it still working?"; stress testing asks "when does it stop working, and how?"

**Q: How often should stress testing be performed?**

A: After significant architecture changes, before major releases, and periodically (quarterly) as a health check. Automated load testing can run more frequently as part of the CI/CD pipeline.

## References

- Ribeiro et al. (2020), "[Beyond Accuracy: Behavioral Testing of NLP Models with CheckList](https://doi.org/10.18653/v1/2020.acl-main.442)", ACL.

- Goel et al. (2021), "[Robustness Gym: Unifying the NLP Evaluation Landscape](https://doi.org/10.18653/v1/2021.naacl-demos.6)", NAACL.

- Wang et al. (2021), "[Adversarial GLUE: A Multi-Task Benchmark for Robustness Evaluation of Language Models](https://arxiv.org/abs/2111.02840)", NeurIPS.
