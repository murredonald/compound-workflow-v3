---
term: "Top-p Sampling"
termSlug: "top-p"
short: "Eine Sampling-Methode, die aus der kleinsten Menge von Tokens auswählt, deren kumulative Wahrscheinlichkeit einen Schwellenwert p übersteigt."
category: "ai-ml"
category_name: "KI & Machine Learning"
related: ["temperature", "top-k", "inference", "llm"]
synonyms: ["Nucleus Sampling", "Top-p Dekodierung", "Wahrscheinlichkeitsmassen-Sampling"]
locale: "de"
draft: false
---

## Definition

Top-p Sampling (auch Nucleus Sampling genannt) ist eine Textgenerierungsstrategie, die dynamisch aus der kleinstmöglichen Menge von Tokens auswählt, deren kumulative Wahrscheinlichkeit einen Schwellenwert p übersteigt. Anders als top-k, das eine feste Anzahl verwendet, passt sich top-p an die Konfidenz des Modells an—wählt weniger Tokens wenn das Modell sicher ist, mehr wenn unsicher.

## Warum es wichtig ist

Top-p bietet intelligente Kontrolle über Output-Diversität:

- **Adaptive Auswahl** — passt Kandidatenpool basierend auf Modellkonfidenz an
- **Qualitätsbalance** — schließt Tokens niedriger Wahrscheinlichkeit aus, die Inkohärenz verursachen
- **Flexibilität** — funktioniert über verschiedene Kontexte ohne manuelles Tuning
- **Komplementär** — kombiniert gut mit Temperatur für feine Kontrolle
- **Produktionsstandard** — Standard-Sampling-Methode in den meisten [LLM](/de/glossary/llm/) APIs

Top-p produziert oft natürlicheren Text als festes Top-k Sampling.

## Wie es funktioniert

```
┌────────────────────────────────────────────────────────────┐
│                   TOP-P (NUCLEUS) SAMPLING                 │
├────────────────────────────────────────────────────────────┤
│                                                            │
│  Token-Wahrscheinlichkeiten (sortiert hoch nach niedrig):  │
│                                                            │
│  Token    Wahrsch.  Kumulativ                              │
│  ─────────────────────────────                             │
│  "der"    0.35      0.35                                   │
│  "ein"    0.25      0.60                                   │
│  "dieser" 0.15      0.75                                   │
│  "jener"  0.10      0.85  ◄── p=0.9 Schwelle              │
│  "das"    0.08      0.93  ◄── inkludiert (übersteigt 0.9) │
│  "mein"   0.04      0.97      ausgeschlossen               │
│  "dein"   0.02      0.99      ausgeschlossen               │
│  "sein"   0.01      1.00      ausgeschlossen               │
│                                                            │
│  ┌────────────────────────────────────────────────┐        │
│  │  TOP-P = 0.9                                   │        │
│  │                                                │        │
│  │  Ausgewählter Nucleus: [der, ein, dieser,     │        │
│  │                         jener, das]           │        │
│  │  Nur aus diesen 5 Tokens samplen              │        │
│  │                                                │        │
│  │  ████████████████████████░░░░░░░░             │        │
│  │  ▲                      ▲                     │        │
│  │  Inkludiert (93%)       Ausgeschlossen (7%)   │        │
│  └────────────────────────────────────────────────┘        │
│                                                            │
│  ADAPTIVES VERHALTEN:                                      │
│  • Sichere Vorhersage → wählt 2-3 Tokens                   │
│  • Unsichere Vorhersage → wählt 10-20 Tokens               │
│                                                            │
└────────────────────────────────────────────────────────────┘
```

**Übliche Top-p Werte:**
| Wert | Verhalten | Anwendungsfall |
|------|-----------|----------------|
| 0.1 | Sehr restriktiv | Deterministische Aufgaben |
| 0.5 | Mäßig fokussiert | Faktische Generierung |
| 0.9 | Ausgewogen (Standard) | Allgemeine Nutzung |
| 0.95 | Diverser | Kreatives Schreiben |
| 1.0 | Alle Tokens | Maximale Diversität |

## Häufige Fragen

**F: Was ist der Unterschied zwischen top-p und top-k?**

A: Top-k wählt immer genau k Tokens unabhängig von ihren Wahrscheinlichkeiten. Top-p wählt eine variable Anzahl basierend auf kumulativer Wahrscheinlichkeit. Top-p passt sich an: wenn ein Token 95% Wahrscheinlichkeit hat, wählt es nur dieses eine; wenn Wahrscheinlichkeiten verteilt sind, wählt es viele.

**F: Was ist ein guter Standard-Top-p-Wert?**

A: 0.9 ist ein üblicher Standard. Er inkludiert die meisten vernünftigen Tokens während der lange Schwanz unwahrscheinlicher Optionen ausgeschlossen wird. Für fokussiertere Ausgabe, versuchen Sie 0.5-0.7; für kreativer, 0.95.

**F: Sollte ich top-p mit Temperatur verwenden?**

A: Ja, sie ergänzen sich. Temperatur formt die Wahrscheinlichkeitsverteilung um; top-p samplet dann aus der angepassten Verteilung. Eine übliche Kombination: Temperatur 0.7 + top-p 0.9.

**F: Bedeutet top-p = 1.0 keine Filterung?**

A: Effektiv ja—alle Tokens sind inkludiert da kumulative Wahrscheinlichkeit immer 1.0 erreicht. Das gibt maximale Diversität, kann aber unsinnige Tokens niedriger Wahrscheinlichkeit enthalten.

## Verwandte Begriffe

- [Temperatur](/de/glossary/temperature/) — formt Wahrscheinlichkeitsverteilung um
- [Top-k Sampling](/de/glossary/top-k/) — Alternative mit fester Anzahl
- [Beam Search](/de/glossary/beam-search/) — andere Dekodierungsstrategie
- [Inferenz](/de/glossary/inference/) — Generierungsprozess

---

## Referenzen

> Holtzman et al. (2020), "[The Curious Case of Neural Text Degeneration](https://arxiv.org/abs/1904.09751)", ICLR. [2.500+ Zitationen]

> Fan et al. (2018), "[Hierarchical Neural Story Generation](https://arxiv.org/abs/1805.04833)", ACL. [1.000+ Zitationen]

> Radford et al. (2019), "[Language Models are Unsupervised Multitask Learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)", OpenAI. [10.000+ Zitationen]

> Welleck et al. (2020), "[Neural Text Generation With Unlikelihood Training](https://arxiv.org/abs/1908.04319)", ICLR. [500+ Zitationen]

## References

> Holtzman et al. (2020), "[The Curious Case of Neural Text Degeneration](https://arxiv.org/abs/1904.09751)", ICLR. [2,500+ citations]

> Fan et al. (2018), "[Hierarchical Neural Story Generation](https://arxiv.org/abs/1805.04833)", ACL. [1,000+ citations]

> Radford et al. (2019), "[Language Models are Unsupervised Multitask Learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)", OpenAI. [10,000+ citations]

> Welleck et al. (2020), "[Neural Text Generation With Unlikelihood Training](https://arxiv.org/abs/1908.04319)", ICLR. [500+ citations]
